\documentclass[11pt]{article}
\usepackage{graphicx,latexsym,amssymb,amsmath}
%% === margins ===
\addtolength{\hoffset}{-0.75in} \addtolength{\voffset}{-0.75in}
\addtolength{\textwidth}{1.5in} \addtolength{\textheight}{1.6in}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}
  {\bf \Large Point-By-Point Reply to the Reviewers}\\
  ``Matching as Nonparametric Preprocessing for Reducing Model
  Dependence in Parametric Causal Inference,'' {\it Political
  Analysis} Manuscript
\end{center}

We begin by thanking the Reviewers for the careful reading of our
manuscript and many helpful comments.  We believe that we have
addressed all of these suggestions and produced our revised
manuscript. In so doing, we have improved our presentation
significantly. In what follows, we detail the changes we made to our
manuscript in response to the editor's and reviewers' comments.

\paragraph{Reply to Reviewer~1}

\begin{enumerate}
\item {\bf Bayesian model averaging and EBA.} In the matching-based
  approach we propose, we are trying to reduce the model uncertainty
  by preprocessing the data. If the matching is done successfully,
  then model uncertainty should be considerably reduced and therefore
  the need for the methods like BMA and EBA will be less. Also, it
  should be noted that matching deals with model uncertainty in the
  specific context of causal inference, whereas BMA and EBA are mainly
  concerned about predictive inference or how well a given model fits
  to the {\it observed} data, as opposed to {\it unobserved potential
    outcomes}.  {\bf Gary, shouldn't we clarify this in the footnote
    we mention BMA and EBA so that we can say it we addressed this
    point in the revision.}

\item {\bf Computing standard errors.} We clarify the issue of how to
  compute the standard errors in the revised manuscript. See the last
  subsection of Section 5. It The standard variance calculation can be
  difficult under the nonparametric settings, but in the context of
  our parametric adjustments, this is straightforward.

\item {\bf Bias-variance trade off.} In the revised manuscript, we
  explicitly discuss the paradoxical advantages of discarding data
  (see Section 5). The basic message is this: discarding the data can
  reduce the variance of the estimator if the model fits the better;
  and hence can reduce the MSE (we already know that the bias goes
  down).  Formally, this is known as the ``self-efficiency'' principle
  in the statistical literature, and the reference is provided in the
  revised manuscript.

\item {\bf Figure 1.} This is meant to be an illustrative rather than
  realistic example. In practice, the detection of model
  misspecification is even more difficult in part due to the
  high-dimensionality of the covariate space.  We noted this point in
  the revised manuscript.

\item {\bf Step-by-step statement.} We did not follow the suggestion
  of providing a ``pseudo-code'' algorithm because we think that
  substantive knowledge researchers have {\it should} be used in the
  process of matching (rather than automating the entire process as
  recommended by some methodologists). But, we tried to state our
  recommendations more clearly in the revised manuscript.

\item {\bf Model dependence.} We formally define model dependence in
  the revised manuscript. See Section 4.3.

\item {\bf Comparison with other approaches.} Our point is that if
  matching is done correctly, then model dependence is reduced and so
  what model you use (whether parametric, semi-parametric, or
  nonparametric) after matching matters little. We clarified this
  point in the revised manuscript.


\item {\bf ``the parametric model.''} We changed ``the'' to `a'
  following the suggestion.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Reply to Reviewer~2}

\begin{enumerate}
\item {\bf Uncertainty about matching procedures.} We disagree with
  the reviewer about the need to account for the uncertainty about
  matching procedures. The uncertainty we are trying to address
  throughout this paper is the uncertainty about model choice rather
  than uncertainty about matching procedures. Matching procedures is
  chosen based on the balance of covariates. Our claim is that after
  matching is done successfully, then model dependence is
  substantially reduced. One may wish to incorporate the uncertainty
  about matching procedure into the final uncertainty estimates
  (though we are not aware of any such method in the literature, and
  if such a method exists, we are happy to reference it), but we
  argue that the uncertainty estimated computed conditional on matched
  observations are also valid for the following reason.

  In virtually all models used in social sciences (i.e.,
  regression-type, and whether parametric or nonparametric, methods),
  the covariates, $X_i$, are considered as fixed and the distribution
  of $X_i$ is not being modeled. The variance is then computed
  conditional on $X_i$.  Therefore, the uncertainty estimates can be
  computed in the same way under the assumed model regardless of
  whether matching or some other procedures (stratification in a
  survey, randomized block designs in experiments) are applied to
  $X_i$. The procedure is valid so long as it does not involve the use
  of the outcome variable.  We made this point clearer in the revised
  manuscript.

\item {\bf One-to-one matching vs other matching procedures.} We do
  not advocate in general one-to-one matching over some other forms of
  matching. In this particular example, however, we find that the
  matching procedures we report in the manuscript (one-to-one matching
  with some exact restrictions) achieves a more satisfactory degree of
  balance than some other matching procedures we tried. Of course,
  this does not mean that some other researchers can come up with a
  matching procedure that achieves even better balance. And if someone
  does, we should use that procedure. One reason why we used
  one-to-one matching in this example rather than some other matching
  methods that retain more observations is that none of the control
  observations in this data set lies in the convex hull of the treated
  observations. Another reason is that there are not many control
  units available relative to the number of the treated units.
  Therefore, we want to be conservative in terms of how many
  observations we retain. We made this point clear in the revised
  manuscript.


\item {\bf Throwing away observations.} We revised the manuscript so
  that it is clearer to readers why throwing away observations can
  result in more efficient estimates. This is formally known as the
  ``sufficiency principle'' in the statistics literature (more data
  are better only when the model is self-sufficient), and the
  reference is provided.

  In the ``Stat 101'' example provided by the reviewer, the selection
  is based on the outcome variable and hence is susceptible to
  possible bias. In contrast, matching is valid because it is only
  conditional on the pre-treatment covariates. We emphasize this key
  point throughout the revised manuscript.

\item {\bf Other matching/subclassification methods.} We agree with
  the reviewer that other matching/subclassification methods are also
  useful if they can improve the balance. We have added references to
  these various methods in the revised manuscript.

\item {\bf Propensity score tautology.} We believe that our
  interpretation of the formal results in Rosenbaum and Rubin (1983)
  as well as the use of the word ``consistency'' are correct. Both
  Theorems~1~and~2 are large sample results rather than finite sample
  ones, and the word ``consistency'' refers to a large sample
  property; the estimator of propensity score converges to the true
  propensity score in probability. The extensions of these theorems to
  general treatment regimes are also based on large sample theory (see
  the Appendix of Imai and van Dyk (2004)).

  The reviewer is correct in that there are balancing scores other than
  the propensity score, which is the coarsest balance score. In fact,
  the covariate matrix $X_i$ is also a balancing score, and this is
  why exact matching works fine. {\bf Gary, we should probably add a
    sentence or two to the manuscript to make the reviewer happy.}

\item {\bf Balance test fallacy.} The logic is most easily understood
  by thinking of an example where one randomly removes the
  observations. In this case, even though the balance has not
  improved, the resulting $p$-values will generally increase due to a
  smaller sample size. We hope that this example, which is explained
  in the manuscript, gives an intuition of why the use of $p$-value
  can be misleading.

\end{enumerate}

\paragraph{Reply to Reviwer~3}

a cranky review.  so we just clarify things a bit more.

authors can keep using their parametric model as they had and they'd
be better off.  of course, we recommend that they also use their
parametric model better, and there's a lot of room for improvement
here.  fair enough, but it doesn't affect us much except for some
wording.

``first to work out the conditions...''  we're right here; the
reviewer found no other citation and neither did we.

I like the Ec Inf section, but we can make it short and make it a FN
perhaps; not sure i think it adds a lot, but maybe we give on things
that aren't central.

'doubly robust'.  i think we need to find a more careful way of making
and appropriately qualifying this claim, but i think we should keep
the phrase.
 
'balance test fallacy':  ok, so we also mention (again!) that we
should verify balance on more than just the first moment.

se's:  right some rare methods don't condition on X.  big deal.  the
clarification i mention above about se's should help here too.

why is this different from 2sls.  ugh.  no reason to change the paper
for this but perhaps we can write something for the reviewer.

exactly what semi-parametric alternatives do social scientists know
well?  

\end{document}

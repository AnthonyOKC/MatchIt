\documentclass[11pt]{article}
\usepackage{graphicx,latexsym,amssymb,amsmath}
%% === margins ===
\addtolength{\hoffset}{-0.75in} \addtolength{\voffset}{-0.75in}
\addtolength{\textwidth}{1.5in} \addtolength{\textheight}{1.6in}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}
  {\bf \Large Point-By-Point Reply to the Reviewers}\\
  ``Matching as Nonparametric Preprocessing for Reducing Model
  Dependence in Parametric Causal Inference,'' {\it Political
  Analysis} Manuscript
\end{center}

Many thanks to the Reviewers for the careful reading of our manuscript
and many helpful comments.  The result, we think, is a much improved
analysis and presentation.  We hope you agree.  In what follows, we
detail the changes we made to our manuscript since the last version.

\paragraph{Reply to Reviewer~1}

\begin{enumerate}
\item {\bf Bayesian model averaging and EBA.} In the matching-based
  approach we propose, we are trying to reduce model uncertainty by
  preprocessing the data.  If the matching is done successfully, then
  model uncertainty should be considerably reduced and therefore the
  need for the methods like BMA and EBA may be reduced, but most
  important is the fact that matching deals with model uncertainty in
  the context of causal inference, which is about estimating
  unobserved potential outcomes.  In contrast, BMA and EBA are
  concerned with predictive inference or how well a given model fits
  the observed data.  These may be very different in the context of
  specific examples, and they remain distinct goals.
  
\item {\bf Computing standard errors.}  The reviews caused us to
  rethink and then in the paper greatly clarify the issue of how to
  compute the standard errors in the revised manuscript.  We highlight
  an important difference of perspectives in the literature.  The
  standard variance calculation can be difficult under the
  nonparametric assumptions, but in the context of the parametric
  models usually used in political science, computing standard errors
  is straightforward.
  
\item {\bf Bias-variance trade off.} The point about discarding data
  is a good one.  In the revised manuscript, we explicitly discuss the
  paradoxical advantages of discarding data (see the new subsection at
  the end of Section 5). The basic message is that discarding the data
  can reduce the variance of the estimator if the model is more
  appropriate to the final data subset; and hence can reduce the MSE
  (since we already know that the bias goes down).  The formal
  statistical issue here is known as the ``self-efficiency''
  principle.  We now reference, describe, and explain this important
  concept.
  
\item {\bf Figure 1.} As we now emphasize better, this figure is meant
  to be illustrative rather than realistic. In practice, the detection
  of model misspecification is more difficult in part due to the
  high-dimensionality of the covariate space and in part because of
  other methodological problems occuring simultaneously.  But in the
  spirit of a controlled experiment, where we change one thing at a
  time for clarity, we use this figure for expository purposes.  (We
  have found it very useful for pedagogical purposes as well.)
  
\item {\bf Step-by-step statement.} We did not follow the suggestion
  of providing a ``pseudo-code'' algorithm because we think that
  substantive knowledge researchers have should be used in the process
  of matching.  There is ongoing unpublished research by us and others
  that seek to automate the entire process, but at least at present we
  have found that using substantive knowledge to match helps improve
  estimates.  We have tried to explicat more clearly exactly how this
  works in the current version of the paper.
  
\item {\bf Model dependence.} As suggested, we now formally define
  model dependence in the revised manuscript. See Section 4.3.
  
\item {\bf Comparison with other approaches.} If matching is done
  correctly, then model dependence is reduced and so whatever
  parametric model or semi-parametric or nonparametric approach one
  chooses after matching matters little: the ultimate causal
  inferences should be approximately the same.  We have tried to
  clarify this point in the revised manuscript.


\item {\bf ``the parametric model.''} We changed ``the'' to `a'
  following the suggestion.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Reply to Reviewer~2}

\begin{enumerate}
\item {\bf Uncertainty about matching procedures.} We appreciate the
  reviewer's point about accounting for the uncertainty of matching
  procedures.  However, one can choose to define uncertainty estimates
  as including any subset of features of the problem that is not known
  for certain; we try to follow current practice.  In virtually all
  models used in social sciences (i.e., regression-type, and whether
  parametric or nonparametric, methods), the covariates, $X_i$, are
  considered as fixed and the distribution of $X_i$ is not modeled.
  The variance is then computed conditional on $X_i$.  This is true
  whether the $X$ and $y$ were sampled jointly or, more relevant to
  the present case, whether the data were sampled conditional on $X$,
  as is commonly the case in stratified sample surveys, randomized
  block designs in experiments, and procedures which subdivide and run
  separate analyses within separate strata of $X$.  The standard
  errors we compute from a parametric analysis using matched data is
  thus valid in the same sense that parametric analysis of unmatched
  data is valid.  We do not disagree with the reviewer that it might
  be interesting and perhaps fruitful to include other sources of
  uncertainty in our confidence intervals and standard errors, but we
  are not aware of any work that does this in the matching literature
  or in other fields, and we have not pursued it.  Almost all social
  scientists condition on $X$ just as we do.  We tried to make these
  points much clearer in the revised manuscript.
  
\item {\bf One-to-one matching vs other matching procedures.} We do
  not advocate in general one-to-one matching over some other forms of
  matching, and have tried to clarify that in the present version. In
  this particular example, however, we find that the matching
  procedures we report in the manuscript (one-to-one matching with
  some exact restrictions) achieves better balance than some other
  matching procedures we tried. Of course, this does not mean that
  some other researchers can come up with a matching procedure that
  achieves even better balance. And if someone does, we should use
  that procedure. One reason why we used one-to-one matching in this
  example rather than some other matching methods is that none of the
  control observations in this data set lies in the convex hull of the
  treated observations. Another reason is that there are not many
  control units available relative to the number of the treated units.
  Therefore, we want to be conservative in terms of how many
  observations we retain.  (These by the way are examples of some of
  the context-specific substantive knowledge useful in matching that
  makes it difficult to automate.) We made this point clear in the
  revised manuscript.

  
\item {\bf Throwing away observations.} We revised the manuscript so
  that it is clearer to readers why throwing away observations can
  result in more efficient estimates. This is formally known as the
  ``self-efficiency principle'' in the statistics literature (more
  data are better only when the model is self-efficient), and the
  reference is provided.
  
  In the ``Stat 101'' example provided by the reviewer, the selection
  is based on the outcome variable and hence is susceptible to
  possible bias. In contrast, matching is valid because it is only
  conditional on the pre-treatment covariates.  This was a helpful
  point to us in revising our manuscript about this key point.

\item {\bf Other matching/subclassification methods.} We agree with
  the reviewer that other matching/subclassification methods are also
  useful if they can improve the balance. We have added references to
  these various methods in the revised manuscript.
  
\item {\bf Propensity score tautology.} We believe that our
  interpretation of the formal results in Rosenbaum and Rubin (1983)
  as well as the use of the word ``consistency'' are correct. Both
  Theorems~1~and~2 are large sample results rather than finite sample
  ones, and the word ``consistency'' of course is a large sample
  property; the estimator of the propensity score converges to the
  true propensity score in probability (if the model is correctly
  specified).  The extensions of these theorems to general treatment
  regimes are also based on large sample theory (see the Appendix of
  Imai and van Dyk (2004)).
  
  The reviewer is correct in that there are balancing scores other
  than the propensity score, which is the coarsest balancing score. In
  fact, the covariate matrix $X_i$ is also a balancing score, and this
  is why exact matching works fine.  We added this helpful point to
  the paper.
  
\item {\bf Balance test fallacy.} The logic is most easily understood
  by thinking of an example where one randomly removes the
  observations. In this case, even though the balance is not improved,
  the resulting $p$-values will increase due to a smaller sample size.
  This will suggest that we are doing better when in fact nothing has
  changed other than discarding useful data.  We hope that this
  example, which is now better explained in the manuscript, gives an
  intuition of why the use of $p$-value can be misleading.

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Reply to Reviewer~3}

\begin{enumerate}
  
\item {\bf Post-treatment bias.} In this manuscript, we focus on the
  most widely accepted model of causal inference in the statistical
  and political methodology literatures (i.e., the one based on the
  potential outcomes). Our recommendation only applies to causal
  models that are valid in this sense. It is well known that in this
  framework, controlling for post-treatment variables can include bias
  (see Rosenbaum, 1984).  Of course, our recommendations will not
  necessarily apply if the purpose of statistical inference is simply
  predictive (e.g., forecasting, etc.) or a different definition of
  causality is employed.
  
\item {\bf The contributions of the paper.} Many methodologists and
  applied researchers used matching methods (followed by
  difference-in-means) instead of common parametric methods. A
  contribution of this paper is to argue that matching can be used in
  conjunction with parametric analysis as a preprocessing procedure.
  We also address a number of important issues including the use of
  hypothesis tests in accessing balance, the use of propensity score
  in matching, and various misunderstandings in and of the theoretical
  literature on matching. We think there are important practical
  implications of our paper too.
  
\item {\bf Connections to missing data and ecological inference.} We
  moved this discussion to Appendix. We want to emphasize that causal
  inference is a very difficult missing data problem because we do not
  observe half of all the potential outcomes. Nonresponse in survey
  and ecological inference are known as difficult missing data
  problems in social sciences, but causal inference can be as or more
  challenging as these difficult problems.  We also added the
  qualifications and clarifications suggested by the reviewer to this
  appendix.

\item {\bf Random vs. fixed causal effects.} The difference between
  the two is whether to treat potential outcomes as fixed quantities
  or random quantities. The former applies to in-sample inferences where
  the probability measure is induced only by random assignment of the
  treatment (as Neyman and Fisher did in their original papers), and
  the latter applies to population inferences where the probability
  measure is induced by both random sampling and random assignment. We
  clarify this point in the revised manuscript.
  
\item {\bf Doubly robust.} This property is a large sample property
  and hence discarding all the observations (or if matching made the
  model unidentifiable) will not achieve the double-robustness.  We
  clarify this issue, and others, and refer interested readers to the
  original work of Jamie Robins.
 
\item {\bf Bias-variance trade-off.} In the revised manuscript, we
  explicitly address this issue.

\item {\bf Balance test fallacy.} We agree with the reviewer that we
  need to check the balance beyond the first moment.  The revised
  manuscript includes this point.

\item {\bf Fixed covariates.} We agree with the reviewer that while
  most social scientists consider the covariates as fixed, some
  methods (whether parametric or nonparametric) model the distribution
  of $X$. We clarified this point by stating more explicitly our
  recommendation about how to compute uncertainty.

\item {\bf Differences between matching and TSLS.} While both are
  two-step estimation procedures, matching and TSLS rely on different
  assumptions. Matching controls for potential confounding variables
  by matching on them, while TSLS controls for them by finding an
  instrument that is independent of them.
  
\item {\bf Sensitivity analysis.} If balance isn't perfect after
  matching, then checking the degree of model dependence after
  matching is well worthwhile, we agree.  The examples we include in
  the paper provide some ways of doing sensitivity analyses to verify
  how much model dependence is left.

\item {\bf Semiparametric methods.} Semiparametric methods do not seem
  to be well known by many social scientists.  The vast majority of
  articles in our journals use parametric regression.  Perhaps this
  should change.

\end{enumerate}

\end{document}

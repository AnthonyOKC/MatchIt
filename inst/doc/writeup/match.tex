\documentclass[11pt,titlepage]{article}
%\usepackage[notref]{showkeys}
\usepackage[reqno]{amsmath}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{comment}
\usepackage{url}
\usepackage[all]{xy}        
\usepackage[usenames]{color}
\newcommand{\Cb}[1]{\textcolor{ProcessBlue}{#1}}  % blue
\newcommand{\Cr}[1]{\textcolor{Red}{#1}}          % red
\newcommand{\Co}[1]{\textcolor{BurntOrange}{#1}}  % orange
\newcommand{\Cg}[1]{\textcolor{Green}{#1}}        % green
\newcommand{\Cp}[1]{\textcolor{Purple}{#1}}       % purple
\newcommand{\Cbr}[1]{\textcolor{RawSienna}{#1}}    % brown
\usepackage{dcolumn}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\usepackage{threeparttable,booktabs}

% changes for the apsr
%\usepackage{apsr}

% Shortcuts
\renewcommand{\P}{\text{P}}
\newcommand{\MC}{\multicolumn}
\usepackage{calc}
\newcounter{hours}\newcounter{minutes}
\newcommand{\printtime}{%
  \setcounter{hours}{\time/60}%
  \setcounter{minutes}{\time-\value{hours}*60}%
  \thehours :\theminutes}
% \Bpara{x coord}{y coord}{rotation angle in degrees}{height in pt}
\newcommand{\Bpara}[4]{\begin{picture}(0,0)%
    \setlength{\unitlength}{1pt}%
    \put(#1,#2){\rotatebox{#3}{\raisebox{0mm}[0mm][0mm]{%
          \makebox[0mm]{$\left.\rule{0mm}{#4pt}\right\}$}}}}%
    \end{picture}}
%
  \title{Matching Methods for Causal Inference}

\author{Daniel E. Ho\thanks{J.D.\ candidate, Yale Law School, Ph.D.\
    candidate, Department of Government, Harvard
    University. (Center for Basic Research in the Social Sciences, 34
    Kirkland, Cambridge MA 02138, USA;
    \texttt{http://www.people.fas.harvard.edu/\~\,deho},
    \texttt{Deho@Fas.Harvard.Edu}).}
\and %
Kosuke Imai\thanks{Assistant Professor, Department of Politics, Princeton
    University (Corwin Hall 041, Department of Politics, Princeton
    University, Princeton NJ 08544, USA;
    \texttt{http://www.princeton.edu/\~\,kimai},
    \texttt{KImai@Princeton.Edu}).}
\and %
Gary King\thanks{David Florence Professor of Government, Harvard
  University (Center for Basic Research in the Social Sciences, 34
  Kirkland Street, Harvard University, Cambridge MA 02138;
  \texttt{http://GKing.Harvard.Edu}, \texttt{King@Harvard.Edu}, (617)
  495-2027).}
\and %
Elizabeth A. Stuart\thanks{Ph.D.\ Candidate, Department of Statistics, Harvard
  University. (Science Center 702, One Oxford Street, Cambridge, MA
  02138, USA;
  \texttt{http://www.people.fas.harvard.edu/\~\,estuart},
  \texttt{Stuart@Stat.Harvard.Edu}).}}

\date{\today\ (\printtime)} 
\begin{document}\maketitle

\begin{abstract}
Randomized experiments have long  been considered the gold standard
for the estimation of causal effects.  Yet due to 
ethical and practical reasons, it is often infeasible to do randomized
experiments in political science.  We 
discuss the ideas of using matching to replicate randomized
experiments using observational data to draw causal inferences.  The main goal is to obtain 
treated and control groups similar to each other on all of the
observed background covariates.  The framework for 
these methods is the Rubin-Holland model for causal inference, which
has penetrated the biomedical sciences
and to a more limited extent the social sciences.  Yet with a few
unpublished exceptions the framework has not been applied to causal
questions in political science, leaving many causal ``answers'' in the
discipline to rest on unfounded functional form assumptions.  
We provide a summary and guidelines to apply matching
to causal questions in political science, with
applications in international relations, American, and comparative
politics, as well as user-friendly ``MatchIt'' software for R / Splus that
implements these matching methods. 
\end{abstract}

\tableofcontents

\clearpage

%\renewcommand{\baselinestretch}{2}
\small
\normalsize

\section{Introduction}

A new and exciting statistics literature has emerged in recent years,
building on advances in nonparametric, non-model-based inference, and
focused around diverse generalizations of different types of matching
estimators.  The potential of this literature is enormous, as it offers
the wherewithall for researchers to make much more reliable causal
inferences than has been the case in most of the social sciences.  These
methods make it possible to vastly reduce the necessary functional form
and other assumptions, and to verify empirically rather than assume
unstated but critical modeling assumptions.
                                                                                                                                                             
Unfortunately, this literature has grown into a cacophony of related but
conflicting techniques, practices, conventions, and rules of thumb.
Valid methods of computing standard errors and confidence intervals are
even more complicated, often not used, and sometimes not available.
Advice and guidelines are lacking or conflicting.
                                                                                                                                                             
In this paper, we attempt to unify this diverse literature for
researchers to make the most productive use of these methods.  Our
simple unifying idea is to think of these new nonparametric techniques
as ways of making the parametric techniques more commonly known and
used in the social sciences (regression, logit, etc.) work better.
This framework thus enables us to add a simple pre-processing step and
let social scientists use whatever models they are already accustomed
to using, rather than requiring them to substitute the methods they
know for one of a set of entirely new and unfamiliar methods.  Our
approach therefore has obvious pedagogical and expository advantages,
but the approach also turns out to be useful in bringing intellectual
order and cohesion to this conflicting literature and to suggest
relatively straightforward advice for empirical researchers seeking to
make causal inferences.  When thought of in the way we propose, the
approach also immediately suggests a simple, standardized, and valid
approach to computing standard errors and confidence intervals for any
of the new techniques.
                                                                                                                                                             
In this paper, we describe our unifying approach and all that which leads
from it.  We also describe software we have prepared and are making
available as a companion to this paper.  The software is easy to use and
implements all the ideas discussed herein.
                                                                                                                                                             
\section{Causal inference and potential outcomes}
\subsection{Fundamentals}
\begin{itemize}
\item causal effects as a comparison of potential outcomes (\cite{Rubin74}): outcome if treatment applied compared with outcome if
treatment not applied (``control'')
\item A simple political science example here?  Whether someone votes if receive a call encouraging them to do so, whether they vote if don't receive a call?
\item Can only observe one potential outcome for each individual (each unit at a particular point in time)...causal inference as a missing data problem
\item Impute the missing potential outcomes, either implicitly or explicitly.  We discuss that explicitly when talking about the analysis.
\item Important to clearly define treatment, units, potential outcomes.  
\end{itemize}


\subsection{Assignment mechanism}
\begin{itemize}
\item So how can we learn about causal effects?
\item Replication...multiple units, some who receive treatment and some who receive control
\item With multiple units, generally make the Stable Unit Treatment Value Assumption (SUTVA) 
\begin{enumerate}
\item Within each treatment group (treatment and control), everyone receives the same treatment--there are no versions of the treatments
\item No interference between units--one unit's treatment assignment does not affect the potential outcomes of any other units
\end{enumerate}
\item Sometimes can help make SUTVA more realistic through design...select units as schools rather than students within schools
\item Assignment mechanism: how the treatments are assigned to the units (i.e., determines which potential outcomes are observed)
\end{itemize}

\subsection{Formal notation}
\begin{itemize}
\item Go into the formal notation here
\item Use notation from KKV: realized causal effect for unit i as $y_{1i} - y_{0i}$.  Generally interested in estimating average realized causal
effect, across individuals in study (or just those in treated group).  These outcomes, $y_{1i}$ and $y_{0i}$ are the outcomes that would be observed
for individual $i$ in this study if individual $i$ gets treatment/control.
\item Make connection to random variables: $Y_{1i}$, $Y_{0i}$ and expected values, $\mu_{1i}$, $\mu_{0i}$
\item Differentiate between finite sample and population inference.  
\item Think of units as randomly drawn from population, so potential outcomes random (but fixed within the experiment, so we are generally thinking
about the realized causal effects).
\item Only one potential outcome observed for each individual.  
Depends on random variable $T_i$, which is the treatment assignment.  Realization in this study denoted by $t_i$.  $y_i = t_i y_{1i} + (1-t_i) y_{0i}$
\item Need to impute values of the missing potential outcomes.  Denoted by $\tilde{y}_{1i}$ or $\tilde{y}_{0i}$.  Draws from posterior distribution. 
\item $X_i$ represent the covariates for individual $i$ (also considered to be random).  In our study we observe realization $x_i$.  
We assume that $x_{0i}=x_{1i}=x_i$ (that covariates $x$ not affected by treatment assignment).
\end{itemize}

\subsection{Unbiasedness of causal estimates in randomized experiments}
\begin{itemize}
\item In randomized experiments, assignment mechanism unconfounded: does not involve the potential outcomes
\item Randomization ensures that covariates balanced between the treated and control groups, and the difference in sample means of the potential outcomes
is an unbiased estimate of the true treatment effect:   $E(\overline{y}_1|t_i=1) - E(\overline{y}_0|t_i=0) = \overline{y}_1 - \overline{y}_0$, where the expectation
is over the randomization distribution. 
\end{itemize} 

\subsection{Replicating a hypothetical randomized experiment with observational data}
\begin{itemize}
\item With observational data, not guaranteed that treated and control groups look similar before treatment assignment
\item Think about replicating a randomized experiment as much as possible (\cite{Rubin91, Rosenbaum99, Rubin01})
\begin{enumerate} \item Find treated and control groups as similar as possible on all background covariates
                  \item Don't use the outcome in the design.  Particularly helpful in high-stakes situations, so that matched samples can't be chosen to
give desired result (e.g., FDA, tobacco litigation).
\end{enumerate}
\item Key assumption: unconfounded treatment assignment: treatment assignment independent of the potential outcomes given the observed covariates,
$ (Y_0,Y_1) \bot T | X$.
\item i.e., for two individuals with same $X$, which treatment each receives is random...within groups defined by the covariates $X$, the treated and control
groups are only randomly different from one another...i.e., replicated a mini-randomized experiment within blocks defined by the covariates
\item \cite{Imbens04} discusses the plausibility of this in economics
\item To make this assumption more plausible, important to include as many covariates as possible in the matching, particularly those related
to both which treatment is received and the outcome
\item \cite{Imbens04} points out that including variables only weakly correlated with treatment assignment and the outcome may reduce precision, but much
research (\cite{RubTho96, Heckman98, Hill04}) shows the importance of including a large set of covariates in the matching procedure.
\item Matching methods developed mostly since 1970's.  Originally looked at bias and variance consequences  of choosing matched rather than random samples for follow-up, when
outcome data has not yet been collected on any units and cost constraints prohibit following up the full samples (\cite{AltRub70, Rubin73a, Rubin73b}).  
\item Methods extended to situation where outcome data already available and interest is in selecting well-matched samples for comparison.  
\end{itemize}

\subsection{Inference}
\subsubsection{Model Adjustments: The Role of Functional Form Assumptions}
\begin{itemize}
\item Standard adjustment for covariates done through modeling (e.g., OLS, logistic regression, etc.)
\item But if not much overlap between the groups, that relies on the modeling assumptions and extrapolation
\item OLS can lead to biased estimates of treatment effect even when relationship between covariates and outcome only moderately non-linear
(\cite{Rubin73a, Rubin73b})
\item In addition, standard use of OLS in matching assumes covariates and the two potential outcomes have parallel linear relationships:
$y_{obs} \sim N(\alpha + \tau t + x \beta, \sigma^2)$ implies $y_1 \sim N(\alpha + \tau + x \beta, \sigma^2)$ and 
$y_0 \sim N(\alpha + x \beta, \sigma^2)$.  In later sections we talk about relaxing this strict relationship, to allow for different
models of the potential outcomes under treatment and control.  
\end{itemize}


\subsubsection{Matching as way to reduce assumptions/extrapolation}
\begin{itemize}
\item Want to predict $y_0$ for the treated group.  If groups far apart, will be doing that prediction using units who don't really look like the treated units. Want to be
able to do that prediction using units with similar covariate values.
\item Matching should make results less sensitive to the modeling assumptions
\end{itemize}

\subsubsection{Benefits to combining matching and modeling}
\begin{itemize}
\item A lot of work has shown the benefits of combining matching and modeling
\item e.g., covariance adjustment on matched samples
\item \cite{Rubin73b, RobRot95, AbaImb04}
\end{itemize}
                                                                                                                                                
\section{Overview of Matching}
\subsection{Main idea: find matches that look as similar as possible}
\subsection{Exact matching}
\subsection{Propensity scores as a way to reduce dimensionality}

\begin{comment}
[From earlier version.]
 Similar to exact matching, propensity score matching is simply a means
to obtain a balanced dataset, and the
variance of the sample ATT is the same as in Equation~\ref{VarATT}.
The only difference is the fact that the missing potential outcome
$\hat{y}_{0i} | t_i=1$ is now estimated by the matched control unit \emph{via the propensity
score}.  Since the propensity score does not estimate any population
parameter and serves its role purely as a ``balancing score'', it does
not introduce any added uncertainty to the variance of the treatment
effect.\footnote{There is one slight exception to this, namely that
  ties in the propensity score are resolved by a random draw.  This is
  a scenario that is likely to happen when the explanatory covariates
  are largely categorical.  Even in this instance, reporting estimates
  from one draw of matched pairs is correct (akin to drawing a random
  sample from a population), but to avoid potentially ``spurious'' findings, the 
  user might well be advised to impute
  missing potential outcomes several times and combine estimates
  across these datasets via standard multiple imputation rules
  (see \citealt{rubin87}).}  (Note, however, that knowledge of the propensity score might decrease the
variance bound for the ATT, but not the ATE
(\citealt{frolich02,hahn98,HirImbRid02}).)  In fact, this variance
estimate might even be conservative for the sample ATT, providing
over-coverage compared to the variance conditional on the
covariates (\citealt{AbaImbND}).
If on the other hand we are interested in the population ATT, the
variance may be estimated by the bootstrap, or even the Neyman
estimate in Note~\ref{neyman} (\citealt{ImbensNDb}, \citealt[Chapter
6]{ImbRubND}).
\end{comment}

                                                                                                                                                             
\section{Details of matching procedures}
\subsection{Estimating propensity scores}
\label{pscorest}
\begin{itemize}
\item Generally must estimate propensity scores...not known (outside of randomized experiments)
\item Propensity scores do not estimate any population
parameter and serves its role purely as a ``balancing score'', so does
not introduce any added uncertainty to the variance of the treatment
effect. 
\item Theoretical and analytic results (\cite{RubTho92b, RubTho96, HilRubTho99}) show that using estimated rather than true propensity scores
can actually lead to lower variance in the estimated treatment effect.  
\item \cite{Hahn98} shows that using estimated propensity scores in the estimation
of the average treatment effect has the same asymptotic efficiency as using the true propensity scores, but that knowing the true propensity score
does lower the asymptotic variance bound of the average effect of the treatment on the treated.  [This isn't great for us, since we are generally in a scenario
of estimating the effect on the treated, since we are matching to the treated group.] 
\end{itemize}

\subsubsection{Logistic regression}
\begin{itemize}
\item Logistic regression usually used to estimate propensity scores.  
\item Fit model with treatment assignment on left-hand side and covariates on right-hand side.
\item Model:  Let $\pi_i=P(t_i=1)$.  Logistic regression model is $log(\frac{\pi_i}{1-\pi_i}) = x_i \beta$. 
\item Fitted values from the logistic regression are the propensity scores: $\hat{\pi}_i = \frac{1}{1+e^{X_i \hat{\beta}}}$
\end{itemize}

\subsubsection{Others: CART, GAM, probit}
\begin{itemize}
\item Other estimation methods can also be used.  [Dan comments here as well?]
\item e.g., probit model: $\pi_i = \Phi(x_i \beta)$, where $\Phi(\mu)$ is the cumulative distribution function of a normal distribution with mean $\mu$ and variance $1$.
\end{itemize}

\subsection{Nearest neighbor matching}
\label{nearest}
\begin{itemize}
\item Generally matching chooses one (or more than one) matches for each unit.  
\item For example, when estimating the average treatment effect on the treated, choose one (or more than one) control unit for each treated unit. 
\item Often done using nearest neighbor matching, which goes through the treated units one by one and finds the best matching control unit. 
\item Variations on and details for this are described in the next few sections
\begin{enumerate} \item  How many matches to get for each treated unit
                  \item  Whether each control unit can be matched to more than one treated unit
\end{enumerate}
\end{itemize}

\subsubsection{Common support: discard}
\begin{itemize}
\item Importance of matching with regions of common support
\item Estimate causal effects only in area of the covariates where there are treated and control units.  
\item This is essentially what matching is always attempting to do, but sometimes done more explicitly.
\item Examples: \cite{KinZen02}, \cite{DehWah99} (control units with propensity 
scores lower than the lowest treated unit propensity score are discarded)
\end{itemize} 

\subsubsection{Order of matching: greedy and optimal matching}
\begin{itemize}
\item The nearest neighbor matching algorithm described in Section \ref{nearest} is a ``greedy'' algorithm
\begin{itemize} \item Chooses the best match for each treated unit one at a time, without trying to reduce a global distance measure
                \item Earlier matches never reconsidered to see if better balance could be obtained.
                \item Need to choose in what order to do that matching: hardest to match first (those with high propensity scores), easiest to match first
(those with low propensity scores), or random order.  
                \item Not a lot of guidance on this order...\cite{Rubin73a} found that the three orders performed similarly
when there is a large pool of control available.  When pool of controls smaller, may want to think about in what part of the covariate space close matches
are particularly desired (those matched first most likely to have better matches).  
\end{itemize}
\item \cite{Rosenbaum02} describes ``optimal'' matching
\begin{itemize} \item Minimizes a global distance measure, thus finding an optimal set of matches.  
                \item Related to network optimization literature.
                \item \cite{GuRos93} find that overall balance generally similar using optimal or greedy matching, 
but that optimal matching does lead to smaller distances between matched pairs
                \item In essence, the two methods (greedy and optimal) are choosing similar sets of matched units, 
but optimal matching does a better job of assigning the matched pairs to each other.  
\end{itemize}
\end{itemize}

\subsubsection{Matching ratios}
\begin{itemize}
\item When many controls available, may be possible to get more than one good match for each treated unit, known as ratio matching. 
\item Will likely have slightly less bias reduction than 1-1 matching, but variance benefits since more units utilized.  
\item \cite{Smith97} discusses these issues in sociology, advocates use of multiple controls.  
\item \cite{Rosenbaum91a} and \cite{MinRos00} discuss the benefits of using variable ratio matching, where the number of control matches for each treated unit varies
\begin{itemize} 
                \item Can lead to very high bias reduction without much loss of precision.
\end{itemize}
\end{itemize}

\subsubsection{With or without replacement}
\begin{itemize}
\item A key issue is whether each control unit can be used as a match multiple times (``with replacement''), or only once (``without replacement'').  
\item Without replacement: after being chosen as a match, that control is taken out of the pool.  With replacement: after being chosen, that control remains in the pool.
\item Matching with replacement can decrease bias because good matches can be used multiple times, but variance estimation more complex since matched units no longer 
independent. 
\item When matching with replacement, the number of times each control is used as a match should be monitored to make sure inference is not based on only a few control units.
\item Matching with replacement can be helpful when the control pool is not very large or there are not very many good matches.  
\item When matching with replacement, variance estimated either using bootstrap (e.g., \cite{AgoDyn04}) or by weighting units in the outcome analysis by the number 
of times they were matched (\cite{DehWah02, HilReiZan04}).
\end{itemize} 

\subsubsection{Caliper matching}
\begin{itemize}
\item Caliper matching (\cite{AltRub70}) selects matches within a specified range (caliper c) of a covariate or set of covariates.  
\item $|x_{ti}-x_{cj}| \leq c$ for all matched pairs, indexed by $j$.  
\item The caliper c usually specified by  the number of standard deviations.  \cite{AltRub70}
showed that with a single covariate, a caliper of 0.2 standard deviations removed nearly all of the bias due to $x$; a caliper of 1 standard deviation removes about
75\% of the bias due to $x$.  \cite{RosRub85a} generally suggest a caliper of about 0.25 standard deviations.
\end{itemize}

\subsection{Subclassification}
\begin{itemize}
\item Subclassification divides the treated and control groups into subclasses with similar covariate values.  
\item \cite{Cochran68} gives analytic results for the effects of subclassification; for a single covariate arising from one of a number of 
continuous distributions (normal, t), creating just five subclasses based on that covariate removes over 90\% of the bias in the estimated treatment 
effect due to imbalance in that covariate. 
\item \cite{RosRub84} shows that creating just five propensity score subclasses removes at least 90\% of the bias in the estimated treatment effect due to each 
of the covariates included in the propensity score specification.  
\item Model-based adjustments can be done within subclasses, and then aggregated across subclasses to obtain an overall
estimated average treatment effect.  
\begin{itemize} \item Weighting the subclass estimates by the number of treated units in each subclass estimates the average treatment effect
for the treated group
                \item Weighting by the overall number of units in each subclass estimates the average treatment effect over the whole population.  
\end{itemize}
\end{itemize}

\subsubsection{Full matching}
\begin{itemize}
\item \cite{Rosenbaum91a} and \cite{Hansen03} discuss ``full matching'', in which the matched sample can be
thought of as being composed of matched sets, where each matched set contains either one treated unit and one or more controls, or one control unit and one or more
treated units.  
\item Can be thought of as the limit of subclassification 
\item Tends to use all of the units in the samples, with no discarding.
\end{itemize}

\subsection{``Special'' variables}
\begin{itemize} 
\item Researcher may want to treat a few variables in a ``special'' way, by obtaining particularly close matches on those variables.
\end{itemize}

\subsubsection{Exact matching on a few variables}
\begin{itemize}
\item When the ``special'' covariates are categorical, the propensity score matching can essentially be done within subclasses defined by those categorical variables
\item Exact matching on the key covariates and then propensity score matching within that.
\end{itemize}

\subsubsection{Mahalanobis matching within propensity score calipers}
\begin{itemize} 
\item Mahalanobis matching on a few key covariates, $x^*$, within propensity score calipers ensures close matches on a few key covariates, but without sacrificing 
close matches on the rest of the covariates 
\item The Mahalanobis distance on covariates $X$ between units $i$ and $j$ is $(x_i-x_j)'\Sigma_{0}^{-1}(x_i-x_j)$, where $\Sigma_0$ is the variance covariance 
matrix of X in the control group.  
\item An algorithm is as follows, from \cite{RubTho00}:
\begin{enumerate}
\item Randomly order the treated units.
\item For the first treated unit, find all available control units with propensity score values that differ from the treated unit's propensity score by less than 
the caliper c.  If there is no such control unit, match the treated unit to the control unit with the nearest value of the propensity score.
\item From the subset of units defined in Step 2, select as a match the control unit who is closest to the treated unit in terms of the Mahalanobis distance
on the covariates $x^*$.  
\item Remove the treated unit and the matched control unit from the lists of treated and control units.  Go to Step 2 for the next treated unit.
\end{enumerate}
\item \cite{RosRub85a} suggest caliper sizes, which depend on the covariate variances in the treated and control groups.  Generally, a caliper of 0.25 standard deviations
of the propensity score is found to work well, however if the variance of the covariates is larger in the treated group than in the control group, a smaller caliper
size may be necessary.  
\item \cite{RubTho00} finds that doing this Mahalanobis matching within propensity score calipers is clearly superior to
matching only on the key covariates or to linear regression adjustment on the full samples, even when the key covariates account for most of the variation in the outcomes.
\end{itemize}

\subsection{Diagnostics}
\begin{itemize}
\item Model diagnostics for propensity score specifications are not the standard model diagnostics (\cite{Rubin04}).  
\item We are not concerned with the parameter estimates themselves, but rather with using the propensity scores as a tool to create well-matched samples. 
\item The specification is assessed by examining covariate balance between the groups and whether the matching is finding groups that are well-matched on all covariates.  
\item Variety of simple diagnostic procedures that can be used, such as testing the difference in means of the covariates and their squares and two-way interactions
between the matched treated and control groups.
\item Because the outcome variable is not used, doing the matching multiple times and choosing the best matched sample will not bias the results, as discussed by \cite{Greevy04}.
\item It may not be possible to obtain good balance on some or all covariates...data may be insufficient for question at hand.  Matching will highlight this
limitation of data, and from there user can either step back and say the data is lacking (e.g., \cite{Rubin01, AgoDyn04}), 
or go forward and estimate causal effects, making it clear that answers are reliant on the model assumptions.
\end{itemize}

\subsubsection{Checking for balance}
\begin{itemize}
\item Even if aren't going to use matched samples, important to check for covariate balance in groups to assess how reliant answers are on modeling assumptions
\item Simple diagnostic is to plot the propensity scores in the treated and control groups; see how much balance there is
\item A few ways to check for balance in the matched samples
\item May want to check for balance in subclasses defined by the propensity scores, especially if analysis done within subclasses.  Matched samples may be unbalanced
overall, but balanced with subclasses.
\end{itemize}

\subsubsection{Bias statistics}
\begin{itemize}
\item Bias statistics: difference in means standardized by covariate standard deviation in treated group
\item For each covariate $X$, compare the original bias, $B = \frac{\overline{x}_{t}-\overline{x}_{c}}{s_{x|t_i=1}}$, with the bias in the matched samples,
$B_m = \frac{\overline{x}_{mt}-\overline{x}_{mc}}{s_{x|t_i=1}}$, where ${\overline{x}_t}$ is the mean of $x$ in the original treated group, $\overline{x}_{mt}$ is
the mean of $x$ in the matched treated group, and $s_{x|t_i=1}$ is the standard deviation of $x$ in the original treated group.  Similar notation holds for the 
control group.
\item Bias should decrease due to the matching.
\end{itemize}

\subsubsection{Means tests}
\begin{itemize}
\item Can also test difference in means of the covariates, their squares, and two-way interactions in the original and matched groups using t-tests.  
\item Should be fewer significant differences in the matched samples, as compared with the original samples.
\item If care about balance in higher level interactions, also test those.
\end{itemize}

\subsubsection{Propensity score model specification}
\begin{itemize}
\item Specific algorithms for diagnosing propensity score models are in \cite{RosRub84}, \cite{Perkins00}, \cite{DehWah02},
\cite{MicBloHil04}, and \cite{matchitdocs}. 
\item Generally involve examining the covariate balance in subclasses defined by the propensity score.  
\item If covariates (or their squares or cross-products) are found to be unbalanced, then those terms are included in the new propensity score specification.  
\item Results show that treatment effect estimates are much more sensitive to mis-specification of the model of the outcome than 
to mis-specification of the propensity score model (\cite{Drake93, DehWah99, DehWah02, Zhao04}).
\end{itemize}

\section{Analysis}
\label{analysis}
\subsection{Average treatment effect, average effect on the treated}

\begin{itemize} 
\item Average treatment effect vs. average
  treatment effect on the treated; 
\item limiting inferences to what's
  scientifically estimable. 
\end{itemize}

Our Assumption
\begin{eqnarray}
  p\,(Y_0 \mid T,  X) = p\,(Y_0 \mid X)
\end{eqnarray}

Our Estimand: population average treatment effect for the treated
\begin{eqnarray}
  E(Y_1 - Y_0 \mid T=1)
\end{eqnarray}

Regression without Matching: assume that we can consistently estimate
the conditional mean function, $\mu_{0}(X)= E(Y_0 \mid X)$, by fitting
a parametric model to the control group.  Then, our estimator is
\begin{eqnarray}
    \frac{1}{N_1} \sum_{i=1}^{N} T_i (Y_{1i} - \hat{\mu}_0(X_i))
\end{eqnarray}
The variance can be estimated by computing the estimate of $V(Y_1 \mid
T=1)$ using the sample variance from the control group, and
$V(\hat{\mu}_0(X_i))$ from the parametric model (via simulation in
Zelig).

Regression with Matching: In the above formulation, we assumed that
$\mu_{0}$ can be consistently computed using the control group and be
used to impute the missing potential outcome for the treatment group.
To make this extrapolation more plausible, we conduct matching to make
the control group similar to the treatment group in terms of
pre-treatment variables. Let $M_i$ represent the number of times the
unit $i$ in the control group is matched with a treated unit.
Matching may be done with replacement.  Now, we estimate $\mu_{0}(X)$
using $M_i$ as weights (e.g., via weighted regression). The variance
can be calculated in the same way as above.

\subsection{"Naive" estimates}
\subsubsection{Neyman's estimates of causal effect, variance}
\subsection{Use modeling to repeatedly impute missing potential outcomes}
\subsubsection{Specify standard models in treated and control groups}
\subsubsection{Logistic regression, OLS, etc. (Zelig)}
\subsubsection{Contrast with standard OLS (parallel linear regressions)}
\subsection{More information on benefits of combining matching and modeling}
\begin{itemize}
\item Many researchers have indicated the benefits of combining matching and modeling adjustments: \cite{Rubin73b, RobRot95, HecHidTod97, AbaImb04}.                          
\item Like using covariance adjustment in randomized experiments to clean up small residual bias.
\item \cite{AbaImb04} propose a bias-corrected estimator, which essentially involves doing covariance adjustment on matched samples
\end{itemize} 

                                                                                                                          
\section{Applications}
\subsection{National Supported Work Demonstration}
\subsection{FDA?}
\subsection{Argentinian?}
\subsection{Cross-sectional?}
\subsection{Panel Data?}
                                                                                                                                                             
 
\section{Open research topics}
\subsection{Multi-treatment regimes}
\subsection{Panel Data}
\begin{itemize}
\item One particularly prevalent issue
  in comparative politics and international relations consists of
  cases where treatment occurs at some time $t$ and persists
  indefinitely.  
\item In Simmons's data, for example, commitment to Article VIII occurs,
  but there is no possibility of ``un-committing.''  In the
  comparative electoral systems data, with few exceptions, a country
  has the same electoral system for long periods of time.  One
  approach taken by researchers has been to define units as
  unit-years, such as country-years or dyad-years.  
\item This, however, violates the
  stability assumptions of matching. 
  [In more traditional econometric terms, the treatment indicator may
  thereby suffer of serial
  correlation\footnote{Duflo, 2003} inducing an artificial causal
  effect.] 
\item We suggest redefining the units to be countries (rather than country/years as done by Simmons and others),
which properly reflects the hypothesized assignment mechanism.  
\item In these examples where treatment occurs at some time
and persists indefinitely, ``treatment assignment" is not done every year for each state, as implied by the usual
treatment of each country and year as a different unit.  
\item Rather, treatment is applied to each state only once and the units modeled should reflect that.  Thus, we recommend [?????] the use of modeling at the country level, 
where
the variables before treatment is applied are treated as covariates and the variables after treatment is applied are treated as outcomes.  
\item This idea is explored by Bertrand et al. (2002), and they find that it has good properties. 
\item Need to define treatment assignment date for the control units.  Discussed by \cite{Rubin91} and \cite{Sianesi04}.
\end{itemize} 

\subsection{Controlling for a post-treatment variable}
\begin{itemize} 
\item We probably want to discuss this a little...
\item Matching or subclassifying on a variable affected by treatment assignment can lead to substantial bias in the estimated treatment effect
(\cite{FraRub02, Greenland03, Imbens04})
\item Mention principal stratification (\cite{FraRub02}); need to subclassify based on pair of post-treatment
variables: the value if treatment received and the value if control received.  
\end{itemize}
                                                                                              
\section{Conclusion}
\subsection{Other issues}
\subsubsection{Missing data}
\begin{itemize} 
\item We have assumed all covariate values observed, so propensity scores estimated using logistic regression.  
\item When covariate values missing, can do something like a general
location model (\cite{DagRub00}) to estimate propensity scores, but much more complex, and diagnostics in particular are difficult.  
\item We advocate creating multiple complete data sets using the ideas behind multiple imputation, 
doing the estimation of causal effects within each data set (i.e., finding matched samples
and estimating the treatment effects), and then combining the results using combining rules. [??????.  I'm not sure about this, but feel that we should mention
missing data--Liz.] 
\item \cite{Song01} provides an example of this approach. 
\end{itemize} 

\subsubsection{Unobserved variables}
\begin{itemize}
\item We have assumed unconfounded treatment assignment.  
\item But what if treatment assignment only unconfounded given some unobserved covariate?  
\item This is a concern with  all observational studies. 
\item Assumption of unconfounded treatment assignment can't be directly tested, although sensitivity tests can be done, to asses how sensitive
results are to a hypothetical unobserved binary covariate.  
\item Details can be found in \cite{RosRub83b} and \cite{Imbens03}.  
\end{itemize}

\subsubsection{Software available}
\begin{itemize} 
\item MatchIT
\item Stata: \cite{AbaDruHerImb02, BecIch02, LeuSia03}
\item SAS: \cite{Dagostino98}
\item Stata and Matlab: \cite{AbaImb04} bias-corrected matching estimators
\end{itemize} 

\subsection{Advice}
\begin{itemize}
\item Choose control groups well..ideally have large pool of controls.  
\item Propensity scores helpful with many covariates.  
\item If pool large enough, get multiple matches for each treated unit.  
\item If groups still unbalanced after matching (e.g., mean propensity scores more than 0.5 standard deviations apart), divide matched samples into
subclasses and do analyses within subclasses, then aggregate (kind of a local-linear/spline approach). 
\item Combine matching with modeling adjustments.  
\end{itemize} 

\subsection{Final thoughts}
\begin{itemize}
\item Stress value in using these tools to diagnose...even if don't use matched samples, good to see how far apart groups are, so know how reliant
answers are on models
\item Lots of value in looking at the plots, etc. generated by MatchIt.
\end{itemize}

\appendix
[These are old...not clear if we still want them.]
\paragraph{Algorithms}
\paragraph{A Sample Matching Algorithm: Optimal Nearest Neighbor One-to-one
Matching with Replacement}
\begin{enumerate}
\item Generate propensity score. \label{assignment}
\item Check balance of pre-treatment covariates.  If an imbalance
  exists, reestimate the assignment model Step~\ref{assignment} by adding
  higher order terms and interactions.  Continue until a obtaining a
  balance in pre-treatment covariates.  
\item Optional: Discard control units with propensity scores below the minimum
  score of treated units (note, other forms of discarding may be suitable).
\item Optional: Re-estimate propensity score.
\item To match units: \label{match}
  \begin{enumerate}
  \item Calculate the absolute difference between propensity
    score of the \emph{non-matched} treated unit with the highest propensity
    score (unit $i$) and all control units. \label{match1}
  \item Match units $i$ and $j$, where:
    \begin{equation}
      | (e_i | t=1)-(e_j | t=0) | < | (e_i | t=1)-(e_k | t=0) |,  \forall k \ne j.  
    \end{equation}
    In the instance multiple control units are
    within the minimum distance, draw randomly to match one of those
    control units. \label{match2}
  \end{enumerate}
\item Continue Steps~\ref{match1}-\ref{match2} for all non-matched
  treated units until all treated units
  are matched. \label{match3}
\item Calculate the average treatment effect on the treated for the
  sample $ATE_s$, where:
  \begin{equation}
    ATE_s=\frac{1}{N_T} \sum_{i=1}^{N_T} \big\{ (y_i|t=1) - (y_m|t=0)
    \big \}
  \end{equation}
where $N_T$ represents the number of treated units, $y_i$ refers to
treated units, and $y_m$ refers to the control unit matched to $i$. 
\item To estimate the point estimate and variance of $ATE$ bootstrap the donor pool $B$ times
  and repeat (Steps~\ref{match}-\ref{match3}) for each bootstrapped sample, storing
  the estimated average treatment effect $ATE_s$ from each bootstrap. 
\item Lastly, calculate quantities of interest from the distribution
  of bootstrapped $ATE_s$, where the point estimate for the average
  treatment effect on the treated is:
\begin{equation}
    ATE = \frac{1}{B} \sum_{s=1}^{B} ATE_s
\end{equation}
and the variance is:
\begin{equation}
    Var(ATE)= \frac{1}{B-1} \sum_{s=1}^{B} (ATE-ATE_s)^2
\end{equation}
\end{enumerate}

\paragraph{The Subclassification/Regression Adjustment Algorithm for a Binary Outcome
  Variable}
\begin{enumerate}
\item Generate propensity score.
\item Construct 10 blocks defined by the deciles of the propensity
  score for the treated group (where to obtain balance
  in the pre-treatment covariates, (a) split imbalanced blocks further, and/or
  (b) respecify the assignment model
  to take into account interactions and squares of imbalanced
  covariates).
\item Discard control units with propensity scores below the minimum
  score of treated units.
\item Re-estimate propensity score and regenerate blocks without the
  discarded units (optional)
\item For units within each substratum: \label{sub}
  \begin{enumerate}
  \item{Estimate the logistic model, regressing
      the binary outcome on the propensity scores and the treatment
      indicator.  (If sample sizes permit, this logistic model may
      also be estimated using all covariates.)} \label{sub1}
  \item{Generate a posterior distribution of the parameters from this
      model, drawing 1000 simulated values of $\beta$, where $\sim$
      represents each simulated set of parameters.}
  \item{For each substratum and one draw of the parameters $\beta$ from the
      posterior distribution, impute the missing potential outcomes
      $\hat{y_i}(1)|t_i=0$ and $\hat{y_i}(0)|t_i=1$ with a logistic regression,
      where:
      \begin{equation}
        (\hat{y_i}(1)|y_i=0) = \frac{1}{1+exp(-x_i^t \beta)}
      \end{equation}
      and 
      \begin{equation}
        (\hat{y_i}(0)|t_i=1) = \frac{1}{1+exp(-x_i^c \beta)}
      \end{equation}
      where the superscripts $c$ and $t$ represents the counterfactual
      treatment indicator of control and treatment, respectively.}
  \item{Calculate the treatment effect for all units 
      in the substratum
      \begin{equation}
        TE_{d} = \left( \begin{array}{c}
            (y(1) | t=1) \\
            (\hat{y}(1) | t=0)
          \end{array} \right)- 
        \left( \begin{array}{c}
            (\hat{y}(0) | t=1)\\
            (y(0) | t=0)
          \end{array} \right)
      \end{equation}
      where $d$ represents the subscript for the decile and $TE_{d}$
      is a vector of treatment effects for all $i$ units in the
      substratum, $(y(1) | t=1)$ and $(y(0) | t=0)$ represent vectors
      of observed potential outcomes, and $(\hat{y}(1) | t=0)$ and
      $(\hat{y}(0) | t=1)$ represent vectors of imputed missing
      potential outcomes.}
  \item {Calculate the average treatment effect for the substratum
      conditional on the set of simulated parameters $ATE_{d,sim}$:
      \begin{equation}
        ATE_{d,sim}=\frac{1}{n_d} \sum_{i=1}^{n_d}TE_{i,d}
      \end{equation}
      where $n_d$ represents the number of units in the substratum,
      and $TE_{i,d}$ refers to the treatment effect for each unit $i$
      in substratum $d$.} \label{sub2}
  \item{Repeat Steps~\ref{sub1}-\ref{sub2} for all draws of $\beta$.}
  \item {Calculate the average treatment effect for the substratum
      across all simulated parameters $ATE_d$:
      \begin{equation}
        ATE_{d}=\frac{1}{1000} \sum_{sim=1}^{1000} ATE_{d,sim}
      \end{equation}}
  \item{Calculate the variance of $ATE_d$ for that substratum:
      \begin{equation}
        Var(ATE_d)=Var(ATE_{d,sim})
      \end{equation}} \label{sub3}
  \item Finally repeat Steps~\ref{sub1}-\ref{sub3} to generate $ATE_d$
    and $Var(ATE_d)$ for all substrata. 
  \end{enumerate}
\item{Having completed each of the above steps for each substratum,
    calculate the overall average treatment effect ATE:
    \begin{equation}
      ATE=\sum_{d=1}^{10} \Big \{ \big (\frac{n_{d,t}}{N_t} \big ) \big (ATE_d
      \big ) \Big \}
    \end{equation}
    where $n_{d,t}$ represents the number of treated units in each
    substratum $d$ and $N_t$ represents the total number of treated units
    across all substrata.}
\item{Finally, calculate the overall variance of the ATE:
    \begin{equation}
      Var(ATE)=\sum_{d=1}^{10} \Big \{ \big (\frac{n_{d,t}}{N_t} \big )^2 \big (Var(ATE_d)
      \big ) \Big \}.
    \end{equation}}
\end{enumerate}



%\begin{figure}[h]
%  \begin{center}
%    \includegraphics[width=3in,angle=-90]{figs/term_lib}
%  \end{center}
%  \caption{Proportion of civil rights and liberties cases decided
%    liberally}
%\end{figure}

\clearpage
%\singlespacing

\bibliographystyle{/home/stuart/gkbibtex/pa}
\bibliography{/home/stuart/gkbibtex/gk,/home/stuart/gkbibtex/gkpubs}

\end{document}

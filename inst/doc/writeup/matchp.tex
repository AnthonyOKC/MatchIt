\documentclass[11pt,titlepage]{article}
%\usepackage[notref]{showkeys}
\usepackage[reqno]{amsmath}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{comment}
\usepackage{url}
\usepackage[all]{xy}        
\usepackage{dcolumn}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\usepackage{threeparttable,booktabs}
%\usepackage{times}
\usepackage{vmargin}
\setpapersize{USletter}
\topmargin=0in

% Shortcuts
\renewcommand{\P}{\text{P}}
\newcommand{\MC}{\multicolumn}
\usepackage{calc}
\newcounter{hours}\newcounter{minutes}
\newcommand{\printtime}{%
  \setcounter{hours}{\time/60}%
  \setcounter{minutes}{\time-\value{hours}*60}%
  \thehours :\theminutes}
%
\title{Matching as Nonparametric Preprocessing\\
for Parametric Causal Inference}

\author{Daniel E. Ho,\thanks{J.D.\ candidate, Yale Law School, Ph.D.\
    candidate, Department of Government, Harvard
    University. (Center for Basic Research in the Social Sciences, 34
    Kirkland, Cambridge MA 02138, USA;
    \texttt{http://www.people.fas.harvard.edu/\~\,deho},
    \texttt{Deho@Fas.Harvard.Edu}).}
%\and %
Kosuke Imai,\thanks{Assistant Professor, Department of Politics, Princeton
    University (Corwin Hall 041, Department of Politics, Princeton
    University, Princeton NJ 08544, USA;
    \texttt{http://www.princeton.edu/\~\,kimai},
    \texttt{KImai@Princeton.Edu}).}
%\and %
Gary King,\thanks{David Florence Professor of Government, Harvard
  University (Center for Basic Research in the Social Sciences, 34
  Kirkland Street, Harvard University, Cambridge MA 02138;
  \texttt{http://GKing.Harvard.Edu}, \texttt{King@Harvard.Edu}, (617)
  495-2027).}
%\and %
Elizabeth A. Stuart\thanks{Ph.D.\ Candidate, Department of Statistics, Harvard
  University. (Science Center 702, One Oxford Street, Cambridge, MA
  02138, USA;
  \texttt{http://www.people.fas.harvard.edu/\~\,estuart},
  \texttt{Stuart@Stat.Harvard.Edu}).}}

\date{\today\ (\printtime)} 
\begin{document}\maketitle

\begin{abstract}
  The fast growing statistical literatures in several disciplines on
  matching methods offer the promise of causal inference without
  resort to the usual difficult-to-justify functional form assumptions
  of commonly used parametric methods.  However, this literature also
  suffers from many diverse and conflicting approaches to estimation,
  uncertainty estimates, theoretical analysis, and practical advice.
  We offer a unified perspective on matching as a method of
  nonparametric preprocessing for the purpose of improving parametric
  methods.  This approach makes it possible for researchers to
  preprocess their data (such as with the easy-to-use matching
  software we have developed) and then to apply whatever familiar
  statistical techniques they would have used anyway.  Under our
  approach, matching does not replace existing methods, but makes them
  work better, giving causal estimates that are considerably less
  model-dependent.
\end{abstract}
\baselineskip=1.57\baselineskip

\section{Introduction}

A new statistics literature has emerged in recent years, building on
advances in nonparametric, non-model-based inference, and focused
around diverse generalizations of different types of matching
estimators for causal inference.  The promise of this literature is
considerable, as it seeks to offer much more reliable causal
inferences than is typical with the standard parametric regressions
social scientists usually run (regression, logit, etc.).  The new
methods make it possible to reduce or eliminate the functional form
and other assumptions of parametric models and to verify empirically
rather than assume critical modeling assumptions.

The statistical literature on matching has grown quite sophisticated,
but from the point of view of the practical researcher, it looks like
a cacophony of related but conflicting techniques, practices,
conventions, and rules of thumb.  Valid methods of computing standard
errors and confidence intervals are even more complicated, often not
used, and sometimes not available.  Coherent guidelines for practice
are conflicting or absent altogether.  Although matching now comprises
a substantial fraction of the empirical work in some disciplines, such
as epidemiology and medicine, the diversity of the substantive
applications, and the difficulties of the conflicting methodological
languages used to describe the same underlying concepts, has limited
the spread of these powerful techniques to much of the social
sciences.

In this paper, we attempt to unify this diverse literature so applied
researchers can make productive use of these new methods.  Our simple
unifying idea is to use these new nonparametric techniques not as
substitutes for our present parametric regression analyses, but
instead to use them to make our familiar parametric techniques work
better.  Under our inferential framework, analysts would merely add a
simple preprocessing step to their data analysis procedures and then
use whatever models they were previously accustomed to using to
analyze the preprocessed data rather than the raw data.  All of the
intuition, diagnostics, and knowledge about these procedures can be
used as before.  

Preprocessing data with matching in the way we propose improves
parametric analyses by making them considerably less dependent on
assumptions about functional forms and distributions.  Our
preprocessing approach has obvious advantages in terms of exposition,
pedagogy, and ease-of-use, but it also turns out to help bring some
order and cohesion to this conflicting literature and to suggest some
relatively straightforward advice for empirical researchers seeking to
make causal inferences.  For example, when thought of in the way we
propose, the approach suggests a simple, standardized, and valid
approach to computing standard errors and confidence intervals for any
of the new techniques.  Our approach has also made it possible for us
to write easy-to-use software that implements all the ideas we discuss
in this paper; it is free and available at
http://GKing.Harvard.edu/zelig.

\section{Definition of Causal Effects}

The notation and ideas in this section parallel that in
\citet[][Section 3.1.1]{KinKeoVer94}, but key aspects of it originate
with many others, especially \citet{Rubin74} and \citet{Holland86}.
The most important idea in this literature is that a causal effect is
a theoretical quantity, defined independently of any empirical method
that might be used to estimate it from real data.  To explicate these
ideas, we use the running example from \citet[][Section
3.1.1]{KinKeoVer94}, where the goal is to estimate the electoral
advantage of incumbency for Democrats in the U.S.\ House of
Representatives.  That is, we only study Democratic incumbents and
define the treatment as when the Democratic Party nominates one of
these incumbents willing to run for a new term and the control as when
the Party nominates someone else to run in an open seat.

The unit of analysis in our example is therefore the congressional
district, which we label with the index $i$ ($i=1,\dots,n$).  In most
of the methodological literature on causal inference, researchers
simplify the exposition by considering only a single dichotomous
causal (or ``treatment'') variable.  We do the same and label it
$t_i$, which takes a value of 1 if unit $i$ receives the treatment and
0 if $i$ is untreated (which is also called the ``control
unit'').\footnote{We will assume that every unit within the treatment
  group receives the same treatment.  This assumption would be
  violated in our running example if the nomination the Democratic
  party gives to the incumbent in some districts means something
  different from that in other districts.  We also assume that the
  treatment in one unit has no effect on the potential outcomes in any
  unit other than its own.  Together these two assumptions are
  sometimes known under the awkward name of the ``stable unit
  treatment value assumption'' or SUTVA.}  In our running example, the
treatment is whether the incumbent is given the party's nomination in
district $i$.  Projects with more complicated causal variables can
dichotomize (perhaps in several alternative ways) or use more
complicated methods \citep{ImaDyk03}.  Those with more than one causal
variable of interest can follow all the advice herein for one variable
at a time.

The outcome (or ``dependent'') variable is $y_i$, which in our case is
the Democratic proportion of the two-party vote in congressional
district $i$.  Finally, prior to the treatment decision, each district
$i$ differs in a variety of ways, some of which we measure and collect
in a vector denoted $X_i$.

To clarify our inferential goals, we begin with by defining the
``realized causal effect,'' which is the simplest definition
available.  To provide further familiarity with this concept, we
briefly show the close connections between this definition and the
goals of the substantively unrelated but mathematically closely
related missing data and ecological inference literatures.  We then
generalize the definition to include features of random causal effects
that are useful for our use of nonparametric methods of preprocessing
for parametric models.  Finally, we give specific causal estimates of
interest at the population level.

\paragraph{Realized Causal Effects}
Because of pretreatment differences among the districts (both
measured, $X_i$, and unmeasured), the causal effect can also differ
across the districts.  As such, the definition of the causal effect
exists at the district level, but the definition does not otherwise
require reference to the pretreatment variables.

A causal effect is a function of \emph{potential outcomes}: let
$y_i(t_i=1)\equiv y_i(1)$ be the vote we would observe in district $i$
in say the 2004 election if in fact the Democratic incumbent received
his or her party's nomination (i.e., $t_i=1$) and $y_i(t_i=0)\equiv
y_i(0)$ the vote the party would receive if the Democratic Party
nominated a nonincumbent ($t_i=0$).  The use of parentheses in this
notation denotes that the outcome is potential, and so not necessarily
observed, and that it depends on the value of the variable in
parentheses.  A key point is that the values of these potential
outcomes remain the same regardless of whether the treatment is in
fact applied in district $i$ or not.  However, since the Democratic
Party either nominated ($t_i=1$) or did not nominate ($t_i=0$) an
incumbent to run in district $i$, we observe either $y_{i}(1)$ or
$y_{i}(0)$ but not both.

The difference between the two potential outcomes defines the simplest
realized (or in-sample) definition of a causal effect:
\begin{equation}
  \label{rce}
  \text{(Realized causal effect for unit $i$)} = y_i(1) - y_i(0).
\end{equation}
The fact that one of these potential outcomes is always a
counterfactual --- and thus is never known for certain no matter how
perfect the research design, experimental control, or number of
observations collected --- expresses what is known as the
``fundamental problem of causal inference'' \citep{Holland86}.

\paragraph{Analogies to Missing Data and Ecological Inference}
If district $i$ receives the treatment, we observe $y_i=y_i(1)$ but
not $y_i(0)$ and otherwise we observe $y_i=y_i(0)$ but not $y_i(1)$.
In this framework, causal inference can be thought of as a severe
missing data problem, where each unit has two relevant dependent
variables, $y_i(0)$ and $y_i(1)$, but one of which, $t_iy_i(0) +
(1-t_i)y_i(1)$, is always missing.  Moreover, since $y_i(0)$ and
$y_i(1)$ are never observed for the same units, we cannot use a known
relationship between the two to extrapolate to units where only one is
observed.  Making causal inferences is thus equivalent to imputing
missing data, using whatever external information is available.  As we
show below, this analogy will prove useful in making the bridge that
connects nonparametric matching to parametric models.

Causal inference can also be thought of as an especially severe form
of ecological inference.  In ecological inference, we observe for each
observation the proportions representing the two marginals of a
$2\times 2$ contingency table (such as the proportion of people voting
and the proportion of people who are black) and the goal is to
estimate the unknown cell proportions (the proportion of blacks who
vote and the proportion of whites who vote).  We make the bridge by
referring to the known ($y_i$ and $t_i$) and unknown ($y_i(0)$ and
$y_i(1)$) quantities in both problems in the same notation and pay
attention to the order in which the information is generated: In both
problems, we imagine that $y_i(0)$ and $y_i(1)$ exist in nature before
our research begins.  Then $t_i$ (the treatment in causal inference or
the row marginal in ecological inference) is applied or otherwise
becomes known.  Finally, we can calculate the observed outcome $y_i$
deterministically via this simple accounting identity:
\begin{equation}
  \label{id}
  y_i = t_iy_i(1) + (1-t_i)y_i(0).
\end{equation}
In ecological inference, by solving (\ref{id}) for one of the unknowns
as a linear function of the other and recognizing that proportions are
always constrained to the unit interval, we can put deterministic
bounds on both quantities of interest with certainty
\citep[][ch.5]{King97}.  In causal inference, we have the advantage of
always knowing one of the quantities exactly; however, because
(\ref{id}) cannot be solved for one of the unknowns (since it would
require dividing by $t_i$ or $1-t_i$, which includes zero) we have the
severe disadvantage of not knowing anything with certainty about the
other unknown.

The fundamental difficulty is the same with causal inference,
statistical inference in the presence of missing data, and ecological
inference: The inferential target is not normally highly constrained
by the observed data and so substantive conclusions will usually
depend on some unverifiable assumptions about the data generation
process.  Laying these assumptions bear as clearly as possible
throughout the process is therefore essential.

\paragraph{Random Causal Effects} In order to make statistical
inferences, we imagine that the realized potential outcomes in
(\ref{rce}) are realizations of corresponding random variables (for
which we use capital letters).  We do not require that the data are
sampled from some specific population, only that there exists some
data generation process that leads to the one realization we see and
could have led to some other realization.  This logic then produces the
random causal effect:
\begin{equation}
  \label{rance}
  \text{(Random Causal Effect for unit $i$)}  = Y_i(1) - Y_i(0)
\end{equation}
features of which we are interested in as alternative quantities of
interest.  For example, our second definition of the causal effect is
the mean causal effect, which is the average over repeated
hypothetical draws of the the random causal effect:
\begin{align}
  \label{meance}
  \text{(Mean Causal Effect for unit $i$)}
  &= E(\text{Random Causal Effect for unit $i$})\\ 
  &= E[Y_i(1) - Y_i(0)]\\ \notag
  &= \mu_i(1) - \mu_i(0),
\end{align}
where $E[Y_i(1)]\equiv\mu_i(1)$ and $E[Y_i(0)]\equiv\mu_i(0)$.

\paragraph{Average Quantities of Interest}
In most applications, we do not attempt to estimate the treatment
effect for each observation, and instead estimate the average over all
observations or some subset of observations.  This leads to two
choices for quantities of interest, for either realized and random
causal effects (we show them here for the latter).  First is the
population average treatment effect or ATE:
\begin{align}
  \label{pate}
  \text{ATE} = E[Y_i(1) - Y_i(0)] \\
             = \frac{1}{n}\sum_{i=1}^n[\mu_i(1) - \mu_i(0)],
\end{align}
which is the mean causal effect for unit $i$ averaged over all units
(so that the expected value operator in the first line averages over
the random potential outcomes for each unit as well as over units).

The alternative choice of a target quantity of interest is the average
treatment effect on the treated, or ATT:
\begin{align}
  \label{att}
  \text{ATT} = E[Y_i(1) - Y_i(0)|t_i=1] \\
             = \frac{1}{\sum t_i}\sum_{i:t_i=1}[\mu_i(1) - \mu_i(0)],
\end{align}
In our running example, this is the average causal effect in districts
in which the Democratic Party nominated the incumbent member of the
House.  From one perspective, we might want to know this treatment
effect on the treated (the ATT) since obviously this is the group of
districts where the treatment was applied.  In other words, the ATT is
the effect of the treatment actually applied.  Medical studies
typically use the ATT as the designated quantity of interest because
they often only care about the causal effect of drugs for patients
that receive or would receive the drugs.  In the social sciences, the
ATT is also a reasonable choice, but so is the average treatment
effect (the ATE).  In our running example, it is quite likely that if
an incumbent were nominated in districts in which he or she did not
actually receive the nomination that an incumbency advantage would
still exist, and whatever this effect is, it might be of substantive
interest.  In this paper, we usually focus on ATT as the quantity of
interest when it is conceptually or algebraically simpler, but we also
show how to compute the ATE.

\section{Data Collection Mechanisms}

We now describe the assumptions necessary for making causal inferences
in experimental and observational research.  Some version of these
assumptions, or some way to deal with the information in them, are
necessary no matter what statistical methods are used for estimation.
Any specific statistical method chosen will make additional
assumptions, but the ones discussed here affect essentially all
methods.

\subsection{Experimental}

Although true experiments are only very rarely conducted in political
science, they remain a useful ideal type for understanding other types
of research.  In addition, some of the assumptions necessary for valid
inference in experiments will be approximated by the preprocessing
procedures we suggest below.

Valid and relatively automatic causal inferences can be achieved via
classical randomized experiments.  Such experiments have three
critical features: (1) \emph{random selection} of units from a given
population to be observed, (2) \emph{random assignment} of values of
the treatment ($t_i$) to each observed unit, and (3) a \emph{large
  $n$}.  

The first feature avoids selection bias by identifying a given
population and guaranteeing that the data generation process is
related to the dependent variable only by random chance.  Combining
this with the large $n$ from the third feature guarantees that the
chance that something will go wrong is vanishingly small.

Random assignment in feature (2) guarantees the absence of omitted
variable bias even without any control variables included.  To see
this, recall that, under the usual econometric rules for omitted
variable bias, a variable $X_i$ must be controlled for if it is
causally prior to $t_i$, empirically related to $t_i$, and affects
$y_i$ conditional on $t_i$.  If, instead, one or more of the three
conditions do not hold, then $X_i$ may be omitted without any bias
resulting.  Random assignment guarantees that $t_i$ is unrelated to
\emph{any} $X_i$, whether measured or not, except by random chance.
Moreover, the large $n$ in condition (3) guarantees that this chance
is vanishingly small.

Experiments are a true ideal type, particularly in relation to most
social science research where almost all research fails to meet at
least one of the three features.  Even most social science lab
experiments have random assignment but no random selection and often a
small $n$.  Traditional survey research has what is intended to be
random selection (although with dramatically increasing nonresponse
rates and cell phone usage, this is a less plausible claim) and
certainly has a large $n$, but random assignment, except when the
treatment is the wording of survey questions, is usually impossible.

\subsection{Observational}

Observational data was generated in a way that does not meet all three
of the features of a classical randomized experiment.  Researchers
trying to use the experimental paradigm try to design research to meet
all three features discussed in the previous section.  Researchers
analyzing observational data are instead forced to make assumptions
that, if correct, help them avoid various threats to the validity of
their causal inferences.

In this paper, we shall assume data are selected in a manner that does
not generate selection bias.  Observations need not be selected at
random, as in an experiment, but the probability of selection must not
be correlated with the dependent variable $y$, after taking into
account the treatment $t$ and pre-treatment covariates, $X$.  This is
not a trivial matter, and it is the subject of a great deal of concern
and study in a large variety of methodological and substantive
literatures.  We mention it here to emphasize that all the well-known
concerns in the literature about selecting on the dependent variable
should remain a concern to researchers even when adopting our approach
to preprocessing data via nonparametric matching procedures.

We also assume that a researcher analyzing observational data has
sufficient information in their measured pre-treatment control
variables $X$ so that it is possible via \emph{some} method to make
valid causal inferences.  This is known in some fields as the absence
of omitted variable bias, so that $X_i$ must include all variables
that are causally prior to $t_i$, associated with $t_i$, and affect
$y_i$ conditional on $t_i$.  In other fields, this same condition is
known as ignorability, which means that $t_i$ and the potential
outcomes are independent after conditioning on $X_i$, and so we can
literally ignore all unobserved variables.  Ignorability is a strong
condition, but it is one about which social scientists are deeply
knowledgeable and which is the central methodological concern of many
substantive scholarly articles.  We emphasize this assumption to make
clear that our procedures contain no magic: They are unable to control
for variables that are not measured.

Assuming ignorability and the absence of selection bias still leaves
many assumptions to be made when choosing a specific statistical
inference method.  Results from these methods can still be biased,
unbiased, efficient, inefficient, and almost anything else, depending
on the analyst's choices.  We now focus on this point in the context
of commonly used parametric methods.

\section{Parametric Analysis Methods}

We begin by specifying a single but general parametric model, special
cases of which include almost all parametric models used regularly in
the social sciences.  First define the stochastic component as $Y_i
\sim p(\mu_i,\theta)$, for a specified probability density $p(\cdot)$,
mean $\mu_i$, and vector of ancillary parameters $\theta$.  Then
denote the systematic component as $E(Y_i|t_i,X_i)\equiv\mu_i=g(\alpha
+ t_i\beta + X_i\gamma)$, for some specified functional form
$g(\cdot)$ and with intercept $\alpha$ and coefficients $\beta$ and
$\gamma$.  The ancillary parameters may also be specified to vary over
observations as a function of $X_i$ or other covariates.  This
framework includes all ``generalized linear models'' as well as many
others.  For example, if $p(\cdot)$ is normal and $g(c)=c$, we have
linear regression; if $p(\cdot)$ is Bernoulli and $g(c)=1/(1+e^{-c})$,
the model reduces to a logistic regression.

We define the ATT in (\ref{att}) under this model by plugging in the
definitions of the potential outcomes from the systematic component,
with $t_i$ taking on values 1 and 0 respectively:
\begin{align}
  \label{matt}
  E(Y_i(1)|t_i=1,X) &\equiv \mu_i(1) = g(\alpha + \beta + X_i\gamma)\notag \\
  E(Y_i(0)|t_i=0,X) &\equiv \mu_i(0) = g(\alpha + X_i\gamma)
\end{align}
We can produce estimates of these quantities by assuming independence
over observations, and forming a likelihood function
\begin{equation}
  \label{lik}
  L(\alpha,\beta,\gamma,\theta|y) = \prod_{i=1}^n 
  p\left(y_i \mid g(\alpha + t_i\beta + X_i\gamma), \theta\right)
\end{equation}
the maximum of which gives parameter estimates.

We now turn to the empirical consequences of having experimental vs.\ 
observational data in making inferences under this model.

\subsection{Experimental Data}\label{s:paraexp}

In experimental data, random assignment guarantees that $T_i$ and (any
observed or unobserved) $X_i$ are independent.  Since stochastic
independence implies mean independence, we can drop $X$ and write
$E(Y_i(1)|t_i=1,X)=E(Y_i(1)|t_i=1)$ and
$E(Y_i(0)|t_i=1,X)=E(Y_i(0)|t_i=1)$.  As such, we can simplify
(\ref{matt}) as
\begin{align}
  \label{smatt}
  E(Y_i(1)|t_i=1) &\equiv \mu_i(1) = g(\alpha + \beta)\notag \\
  E(Y_i(0)|t_i=0) &\equiv \mu_i(0) = g(\alpha)
\end{align}
and so the average treatment effect on the treated from (\ref{att})
becomes simply
\begin{equation}
  \label{satt}
  \text{ATT} = g(\alpha+\beta) - g(\alpha),
\end{equation}
which most importantly no longer has a summation sign over $i$.

The systematic components in Equations \ref{smatt} are now scalar
constants for all $i$.  This is a key result, since it means that
changing the definition of the functional form $g(\cdot)$ now amounts
to nothing more than a simple reparameterization.  Fortunately,
maximum likelihood is invariant to reparameterization
\citep[][p.75-6]{King89}, which means that no matter how $g(\cdot)$ is
defined we get the same estimate of the expected potential outcomes.
Moreover, given any chosen stochastic component, this result holds for
virtually any parametric model, including all the special cases of the
general model given above.

Since the specific maximum likelihood estimator of the population mean
for most common probability densities is merely the sample mean, the
analysis of classical randomized experiments often comes down to
taking the difference in the sample means of $y$ for the treatment and
control groups.  But even if one chooses to run a parametric model,
the absence of model dependence means that it will not matter: The
results will be the same no matter what the choice for $g()$ is.

\subsection{Observational Data} \label{s:paraobs}

In experiments, then, random assignment breaks the link between $T$
and $X$ and, by so doing, eliminates the problem of model dependence.
When analyzing observational data with parametric methods we are not
so fortunate.  We cannot reduce Equations \ref{att} to Equations
\ref{matt} and so are left having to model the full functional
relationship that connects the mean as it varies as a function of $t$
and $X$ over observations.  Since $X$ is typically multidimensional,
this is a task far more difficult and highly dependent on often
unstated assumptions than many realize.

To see this point, which is known as the curse of dimensionality,
suppose for simplicity that we have a dependent variable and one
ten-category explanatory variable, and our goal is use linear
regression technology to estimate the functional relationship without
functional form assumptions.  To do this, we would represent the ten
categories with ten parameters (a constant and nine dummy variables or
10 mean indicator variables).  In contrast, the usual approach to
estimation is to assume linearity.  This enables us to enter not 10
indicator variables, but rather only a constant term and one slope
coefficient.  How do we get from ten parameters to only two?  Pure
assumption about the remaining eight.  If we have some sense that the
relationship is indeed linear or close to linear, this is a good use
of external information to reduce the number of parameters that must
be estimated.  If not, then we still have the best linear
approximation to the conditional expectation function, but the
relationship we estimate can be far off.  If we are running this
regression for the purpose of estimating a causal effect, then the
treatment variable is also in the regression, and its coefficient can
be biased to any degree if the functional relationship with the
control variables is misspecified.

This problem quickly becomes more serious as the number of explanatory
variables increase.  For example, estimation without functional form
assumptions with two ten-category explanatory variables would require
not 20 parameters but 100.  In this case, the usual approach would
include a constant term and two slope coefficients, reducing 100
parameters to three by pure assumption.  And with multiple explanatory
variables, claims about external knowledge constraining the functional
form much become dubious.  In this example, by what theory would we
know that 97 parameters, representing every form of nonlinearity and
interaction, should be set to exactly zero?  Including a linear
interaction would not help much since it would merely add one more
parameter to estimate, and so we would still need to make assumptions
about the remaining 96 parameters.

The problem escalates very fast as the number of explanatory variables
increase.  With $v$ 10-category variables, we need $10^v$ parameters
for estimation in a regression framework without functional form
assumptions.  So five explanatory variables leads to a regression
model with 100,000 parameters, and the usual approach works by
estimating only a constant term and five slope coefficients and
restricting by assumption the remaining 99,994 parameters.  Nine such
explanatory variables leaves us with a model with one \emph{billion}
parameters, to be approximated by only the ten that we would estimate
under the traditional approach.  Eighty ten-category explanatory
variables would require a regression model with more parameters than
current estimates of the number of elementary particles in the
universe.

Estimating rather than making assumptions about all these extra
parameters is obviously not possible under the standard regression
approach, since social data sets do not come with anywhere near enough
observations.  We cannot avoid the problem with nonlinear or
non-normal statistical models, since these pose the same curse of
dimensionality as linear regression.  The assumption of ignorability,
which enables us to make the positivist assumption that we have have
measured and observe all necessary variables, is insufficient.

Instead, we are led to the inescapable conclusion that, in parametric
causal inference of observational data, many assumptions about many
parameters are frequently necessary, and only rarely do we have
sufficient external information to make these assumptions based on
genuine knowledge.  The frequent unavoidable consequence is high
levels of model dependence, but no good reason to choose one set of
assumptions over another.  

Some researchers surely respond to this diversity of possible models
by inadvertently choosing specifications that support their favored
hypotheses.  The best scholars forthrightly portray at least some
aspects of specification uncertainty in their published work by
displaying the results from multiple specifications and evaluating and
emphasizing how model dependent their substantive results are.  But
researchers rarely portray the full sensitivity of their causal
inferences to model specification, and attempts to do so in many
situations lead to nihilistic conclusions.\footnote{Two approaches to
  this problem include extreme bounds analysis \citep{Leamer78}, which
  is a relatively standardized way to portray model dependence, and
  Bayesian model averaging \citep{HoeMadRaf99,ImaKin04}, which is
  intended to draw inferences by appropriately combining inferences
  from models with different specifications.}

\section{Nonparametric Preprocessing}

The goal of matching in general and our specific nonparametric
preprocessing approach is to adjust the data prior to the parametric
analysis so that (1) the relationship between $t_i$ and $X_i$ is
eliminated or reduced and (2) no bias is induced.  If we are able to
adjust the data so that $t_i$ and $X_i$ are completely unrelated, we
will have moved a good deal of the way from Section \ref{s:paraobs} to
Section \ref{s:paraexp}.  An assumption of ignorability will still be
necessary, but we will no longer need to model the full parametric
relationship between the dependent variable and the multidimensional
$X_i$.  We will also have eliminated an important source of model
dependence in the resulting parametric analysis stemming from the
functional form specification and the curse of dimensionality.  For
data sets where preprocessing reduces the extent of the relationship
between $t_i$ and $X_i$, but is unable to make them completely
independent, model dependence is not eliminated but will normally be
greatly reduced.  Indeed, if nonparametric preprocessing results in no
reduction of model dependence, then it is likely that the data have
little information to support causal inferences by any method, which
of course would also be useful information.

But how can we adjust the data without inducing bias in our causal
estimates?  The key to this problem is that the fundamental rule for
avoiding selection bias --- do not select on the dependent variable
--- does not prevent us from selecting observations on the explanatory
variables ($t_i$ or $X_i$).  Random or other physical assignments in
experiments and stratified sampling in surveys are examples of data
collection mechanisms that select observations given chosen values of
the explanatory variables.  But we can also select observations, or
selectively drop observations from an existing sample without bias, as
long as we do so using a rule that is a function only of $t_i$ and
$X_i$.  Our preprocessed dataset will therefore include a selected
subset of the observed sample for which $t_i$ and $X_i$ are unrelated,
or in other words for which this relationship holds:
\begin{equation}
  \label{balance}
  p(X|t=1) = p(X|t=0).
\end{equation}

The simplest way to satisfy (\ref{balance}) is to use \emph{one-to-one
  exact matching}.  The idea is to match each treated unit with one
control unit for which the values of $X_i$ are identical.  Our
preprocessed dataset thus is the same as the original dataset with any
unmatched units discarded, and $t_i$ and $X_i$ are now independent.
If enough matches are available, this procedure eliminates all
dependence in the parametric analysis on the functional form.  It is
also highly intuitive, since it directly parallels an experiment where
we find pairs of units that are identical in all observable ways and
assign one from each pair to be treated and the other to be a control.
Then no matter what effect $X_i$ has on $y$, we can ignore it entirely
since $X_i$ is literally held constant within each pair of units.

Although one-to-one exact matching can eliminate model dependence and
any bias from incorrect assumptions made during the parametric stage
of analysis, it has the disadvantage in many applications of not
generating many matches.  The problem is of course most severe if
$X_i$ is high dimensional or contains continuous variables.  The
result may then be a preprocessed data set with very few observations
that results in a parametric analysis with large standard errors.  In
this situation, we may have reduced model dependence and the potential
for bias but decreased efficiency and as a result mean square error.
If this occurs in practice, those in the matching literature tend to
sacrifice some bias reduction for the increased efficiency that comes
from having more observations in the preprocessed dataset.  In our
approach, sacrificing bias reduction only affects the preprocessing
stage, and our second stage parametric analysis has a chance to
eliminate the remaining bias.  We therefore now turn to a menu of
matching procedures that enable researchers to satisfy (\ref{balance})
as closely as possible while still generating a preprocessed dataset
with enough observations.

\section{Choosing a Matching Procedure}

\section{Computing Uncertainty Estimates}

No concensus exists in the literature on methods to use to compute
uncertainty estimates, such as standard errors or confidence
intervals, from matching procedures.  The problem is not some
disagreement exists over technical statistical techniques.  Rather,
the issue revolves around normative criteria such as what researchers
should condition on and what they should attribute to additional
uncertainty.  Since scholars are no more likely to reach consensus via
debate on normative statistical issues than on normative political
issues, we believe the best way forward is to choose a reasonable
definition and show how to compute uncertainty estimates for it, and
let others pick different definitions if they prefer.  Our choice,
which we now explicate, appears substantively reasonable and has the
advantage of being easy to implement.

Parametric methods applied to raw data without preprocessing come with
a list of relatively standard ways of computing uncertainty estimates.
These include methods based on using the asymptotic normal
approximation to the likelihood function, direct simulation from the
finite sampling distribution or posterior density, various frequentist
bias corrections, robust Bayesian analysis involving classes of
posteriors, and even nonparametric bootstrapping, among others.  Any
of these are easy to implement by drawing and summarizing simulations
of the chosen quantity of interest, given a parametric model and
accompanying theory of inference.

Our idea is to take advantage of a common feature of the uncertainty
estimates associated with parametric methods: They are all conditional
on the pretreatment variables $X$ (and $t$), which are therefore
treated as fixed and exogenous.  Since our preprocessing procedures
modify the raw data only in ways that are solely a function of $X$, a
reasonable method of defining uncertainty for the purpose of computing
uncertainty estimates is to continue to treat $X$, and thus our entire
preprocessing procedures, as fixed.  The advantage of this definition
is that we can easily compute standard errors and confidence intervals
using the same methods researchers have been using with their
parametric methods all along, but applied to the preprocessed instead
of raw data.  (The one exception to our procedure occurs when matching
with replacement, where we would then run the parametric procedure
weighted by the number of treated units matched to each control unit.)

Thus, when estimating the ATT or ATE, we compute estimates of
$\mu_i(1)$ and $\mu_i(0)$ and their uncertainty as usual from the
parametric model applied to the preprocessed data.  If computing the
realized causal effect (either on average over all observations or
just the average for the treated units), we compute parametric
estimates conditional on the observed $y_i$ for each unit.  In this
situation, if $t_i=1$, we set $\mu_i(1)=y_i$ and use the parametric
model to estimate $\mu_i(0)$ and its uncertainty, whereas if $t_i=0$,
we set $\mu_i(0)=y_i$ and use the parametric model to estimate
$\mu_i(1)$ and its uncertainty estimate.  In Bayesian models,
estimating the realized causal effect requires the equivalent of
conditioning on $y_i$ after the likelihood estimation (as in
\citet{King97}).

\section{Empirical Illustrations}

\section{What Can Go Wrong}

\section{Concluding Remarks}

\appendix
\section{Matching Software}

\baselineskip=0.637\baselineskip
\bibliographystyle{apsr} 
\bibliography{gk,gkpubs}

\end{document}

\documentclass[11pt,titlepage]{article}
%\usepackage[notref]{showkeys}
\usepackage[reqno]{amsmath}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{comment}
\usepackage{url}
\usepackage[all]{xy}        
\usepackage{dcolumn}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\usepackage{threeparttable,booktabs}
%\usepackage{times}
\usepackage{vmargin}
\setpapersize{USletter}
\topmargin=0in

% Shortcuts
\renewcommand{\P}{\text{P}}
\newcommand{\MC}{\multicolumn}
\usepackage{calc}
\newcounter{hours}\newcounter{minutes}
\newcommand{\printtime}{%
  \setcounter{hours}{\time/60}%
  \setcounter{minutes}{\time-\value{hours}*60}%
  \thehours :\theminutes}
%
\title{Matching as Nonparametric Preprocessing\\
for Parametric Causal Inference}

\author{Daniel E. Ho,\thanks{J.D.\ candidate, Yale Law School, Ph.D.\
    candidate, Department of Government, Harvard
    University. (Center for Basic Research in the Social Sciences, 34
    Kirkland, Cambridge MA 02138, USA;
    \texttt{http://www.people.fas.harvard.edu/\~\,deho},
    \texttt{Deho@Fas.Harvard.Edu}).}
%\and %
Kosuke Imai,\thanks{Assistant Professor, Department of Politics, Princeton
    University (Corwin Hall 041, Department of Politics, Princeton
    University, Princeton NJ 08544, USA;
    \texttt{http://www.princeton.edu/\~\,kimai},
    \texttt{KImai@Princeton.Edu}).}
%\and %
Gary King,\thanks{David Florence Professor of Government, Harvard
  University (Center for Basic Research in the Social Sciences, 34
  Kirkland Street, Harvard University, Cambridge MA 02138;
  \texttt{http://GKing.Harvard.Edu}, \texttt{King@Harvard.Edu}, (617)
  495-2027).}
%\and %
Elizabeth A. Stuart\thanks{Ph.D.\ Candidate, Department of Statistics, Harvard
  University. (Science Center 702, One Oxford Street, Cambridge, MA
  02138, USA;
  \texttt{http://www.people.fas.harvard.edu/\~\,estuart},
  \texttt{Stuart@Stat.Harvard.Edu}).}}

\date{\today\ (\printtime)} 
\begin{document}\maketitle

\begin{abstract}
  The fast growing statistical literatures on matching methods in
  several disciplines offer the promise of causal inference without
  resort to the usual difficult-to-justify functional form assumptions
  of commonly used parametric methods.  However, this literature also
  suffers from many diverse and conflicting approaches to estimation,
  uncertainty, theoretical analysis, and practical advice.  In this
  paper, we propose a unified perspective on matching as a method of
  nonparametric preprocessing for improving parametric methods.  This
  approach makes it possible for researchers to preprocess their data
  (such as with the easy-to-use matching software we have developed)
  and then to apply whatever familiar statistical techniques they
  would have used anyway.  Under our approach, instead of using
  matching to replace existing methods, we use it to make existing
  methods work better, such as by giving more accurate and
  considerably less model-dependent causal inferences.
\end{abstract}
\baselineskip=1.57\baselineskip

\section{Introduction}

A new statistics literature has emerged in recent years, building on
advances in nonparametric, non-model-based inference, and focused
around diverse generalizations of different types of matching
estimators for causal inference.  The promise of this literature is
considerable, as it seeks to offer much more reliable causal
inferences than is typical with the standard parametric regressions
social scientists usually run (regression, logit, etc.).  The new
methods make it possible to reduce or eliminate the functional form
and other assumptions of parametric models and to verify empirically
rather than assume critical modeling assumptions.

The statistical literature on matching has grown quite sophisticated,
but from the point of view of the practical researcher, it looks like
a cacophony of related but conflicting techniques, practices,
conventions, and rules of thumb.  Valid methods of computing standard
errors and confidence intervals are even more complicated, often not
used, and sometimes not available.  Coherent guidelines for practice
are conflicting or absent altogether.  Although matching now comprises
a substantial fraction of the empirical work in some disciplines, such
as epidemiology and medicine, the diversity of the substantive
applications, and the difficulties of the conflicting methodological
languages used to describe the same underlying concepts, has limited
the spread of these powerful techniques to much of the social
sciences.

In this paper, we attempt to unify this diverse literature so applied
researchers can make productive use of these new methods.  Our simple
unifying idea is to use these new nonparametric techniques not as
substitutes for our present parametric regression analyses, but
instead to use them to make our familiar parametric techniques work
better.  Under our inferential framework, analysts would merely add a
simple preprocessing step to their data analysis procedures and then
use whatever models they were previously accustomed to using to
analyze the preprocessed data rather than the raw data.  All of the
intuition, diagnostics, and knowledge about these procedures can be
used as before.

Preprocessing data with matching in the way we propose improves
parametric analyses by making them considerably less dependent on
assumptions about functional forms and distributions.  Our
preprocessing approach has obvious advantages in terms of exposition,
pedagogy, and ease-of-use, but it also turns out to help bring some
order and cohesion to this conflicting literature and to suggest some
relatively straightforward advice for empirical researchers seeking to
make causal inferences.  For example, when thought of in the way we
propose, the approach suggests a simple, standardized, and valid
approach to computing standard errors and confidence intervals for any
of the new techniques.  Our approach has also made it possible for us
to write easy-to-use software that implements all the ideas we discuss
in this paper; it is free and available at
http://gking.harvard.edu/matchit.

\section{Definition of Causal Effects}

The notation and ideas in this section parallel that in
\citet[][Section 3.1.1]{KinKeoVer94}, but key aspects of it originate
with many others, especially \citet{Rubin74} and \citet{Holland86}.
The most important idea in this literature is that a causal effect is
a theoretical quantity, defined independently of any empirical method
that might be used to estimate it from real data.  To explicate these
ideas, we use the running example from \citet[][Section
3.1.1]{KinKeoVer94}, where the goal is to estimate the electoral
advantage of incumbency for Democrats in the U.S.\ House of
Representatives.  That is, we only study Democratic incumbents and
define the treatment as when the Democratic Party nominates one of
these incumbents willing to run for a new term and the control as when
the Party nominates someone else to run in an open seat.

The unit of analysis in our example is therefore the congressional
district, which we label with the index $i$ ($i=1,\dots,n$).  In most
of the methodological literature on causal inference, researchers
simplify the exposition by considering only a single dichotomous
causal (or ``treatment'') variable.  We do the same and label it
$t_i$, which takes a value of 1 if unit $i$ receives the treatment and
0 if $i$ is untreated (the ``control condition'').\footnote{We will
  assume that every unit within each treatment group receives the same
  treatment.  This assumption would be violated in our running example
  if the nomination the Democratic party gives to the incumbent in
  some districts means something different from that in other
  districts.  We also assume that the treatment in one unit has no
  effect on the potential outcomes in any unit other than its own.
  Together these two assumptions are sometimes known under the awkward
  name of the ``stable unit treatment value assumption'' or SUTVA
  \citep{Rubin74}.}  In our running example, the treatment is whether
the incumbent is given the party's nomination in district $i$.
Projects with more complicated causal variables can dichotomize
(perhaps in several alternative ways) or use more complicated methods
\citep{ImaDyk03}.  Those with more than one causal variable of
interest can follow all the advice herein for one variable at a time.

The observed outcome (or ``dependent'') variable is $y_i$, which in
our case is the Democratic proportion of the two-party vote in
congressional district $i$.  Finally, prior to the treatment decision,
each district $i$ has a variety of characteristics, some of which we
measure and collect in a vector denoted $X_i$.

To clarify our inferential goals, we begin with by defining the
``realized causal effect,'' which is the simplest definition
available.  To provide further familiarity with this concept, we
briefly show the close connections between this definition and the
goals of the substantively unrelated but mathematically closely
related missing data and ecological inference literatures.  We then
generalize the definition to include features of random causal effects
that are useful for our use of nonparametric methods of preprocessing
for parametric models.  Finally, we give specific causal estimates of
interest at the population level.

\paragraph{Realized Causal Effects}
Because of pretreatment differences among the districts (both
measured, $X_i$, and unmeasured), the causal effect can also differ
across the districts.  As such, the definition of the causal effect
exists at the district level, but the definition does not otherwise
require reference to the pretreatment variables.

A causal effect is a function of \emph{potential outcomes}: let
$y_i(t_i=1)\equiv y_i(1)$ be the vote we would observe in district $i$
in say the 2008 election if in fact the Democratic incumbent receives
his or her party's nomination (i.e., $t_i=1$) and $y_i(t_i=0)\equiv
y_i(0)$ the vote the party would receive if the Democratic Party
nominates a nonincumbent ($t_i=0$).  The use of parentheses in this
notation denotes that the outcome is potential, and so not necessarily
observed, and that it depends on the value of the variable in
parentheses.  A key point is that the values of these potential
outcomes remain the same regardless of whether the treatment is in
fact applied in district $i$ or not.  However, since the Democratic
Party either will nominate ($t_i=1$) or will not nominate ($t_i=0$) an
incumbent to run in district $i$, we will observe either $y_{i}(1)$ or
$y_{i}(0)$ but not both.

The difference between the two potential outcomes defines the simplest
realized (or in-sample) definition of a causal effect:
\begin{equation}
  \label{rce}
  \text{(Realized causal effect for unit $i$)} = y_i(1) - y_i(0).
\end{equation}
The fact that one of these potential outcomes is always a
counterfactual --- and thus is never known for certain no matter how
perfect the research design, experimental control, or number of
observations collected --- expresses what is known as the
``fundamental problem of causal inference'' \citep{Holland86}.

\paragraph{Analogies to Missing Data and Ecological Inference}
If district $i$ receives the treatment, we observe $y_i=y_i(1)$ but
not $y_i(0)$ and otherwise we observe $y_i=y_i(0)$ but not $y_i(1)$.
In this framework, causal inference can be thought of as a severe
missing data problem, where each unit has two relevant dependent
variables, $y_i(0)$ and $y_i(1)$, but one of which, $t_iy_i(0) +
(1-t_i)y_i(1)$, is always missing.  Moreover, since $y_i(0)$ and
$y_i(1)$ are never observed for the same units, we cannot use a known
relationship between the two to extrapolate to units where only one is
observed.  Making causal inferences is thus equivalent to imputing
missing data, using whatever external information is available.  As we
show below, this analogy will prove useful in making the bridge that
connects nonparametric matching to parametric models.

Causal inference can also be thought of as an especially severe form
of ecological inference.  In ecological inference, we observe for each
observation the proportions representing the two marginals of a
$2\times 2$ contingency table (such as the proportion of people voting
and the proportion of people who are black) and the goal is to
estimate the unknown cell proportions (the proportion of blacks who
vote and the proportion of whites who vote).  We make the bridge by
referring to the known ($y_i$ and $t_i$) and potentially unknown
($y_i(0)$ and $y_i(1)$) quantities in both problems in the same
notation and pay attention to the order in which the information is
generated: In both problems, we imagine that $y_i(0)$ and $y_i(1)$
exist in nature before our research begins.  Then $t_i$ (the treatment
in causal inference or the row marginal in ecological inference) is
applied or otherwise becomes known.  Finally, we can calculate the
observed outcome $y_i$ deterministically via this simple accounting
identity:
\begin{equation}
  \label{id}
  y_i = t_iy_i(1) + (1-t_i)y_i(0).
\end{equation}
In ecological inference, by solving (\ref{id}) for one of the unknowns
as a linear function of the other and recognizing that proportions are
always constrained to the unit interval, we can put deterministic
bounds on both quantities of interest with certainty
\citep[][ch.5]{King97}.  In causal inference, we have the advantage of
always knowing one of the quantities exactly; however, because
(\ref{id}) cannot be solved for one of the unknowns (since it would
require dividing by $t_i$ or $1-t_i$, which includes zero) we have the
severe disadvantage of not knowing anything with certainty about the
other unknown.

Social scientists correctly think of missing data imputation and
ecological inference as difficult and hazardous areas of statistical
inference.  After all, both involve making inferences about patterns
in unobserved quantities by using patterns in observed data, with
nothing but optimistic assumptions guaranteeing that the former have
anything necessarily to do with the latter.  However, scholars should
be no less concerned when making causal inferences, as every causal
effect ever estimated involves essentially identical inferential
hazards.  If anything, the specific problems in making causal
inferences are even more difficult than in these other areas.
Moreover, whether using linear regressions, simple descriptive
differences in means, or complicated multiple equation estimators,
causal inference requires assumptions that are no easier to justify
than those in imputation or ecological inference.  In all three areas
the inferential target is not normally highly constrained by the
observed data and so substantive conclusions will usually depend on
unverifiable assumptions about the data generation process.  And so
laying these assumptions bear as clearly as possible throughout the
process is essential.

\paragraph{Random Causal Effects} In order to make statistical
inferences, we imagine that the realized potential outcomes in
(\ref{rce}) are realizations of corresponding random variables (for
which we use capital letters).  We do not require that the data are
sampled from some specific population, only that there exists some
data generation process that leads to the one realization we see and
could have led to some other realization.  This logic then produces
the random causal effect:
\begin{equation}
  \label{rance}
  \text{(Random Causal Effect for unit $i$)}  = Y_i(1) - Y_i(0),
\end{equation}
features of which we are interested in as alternative quantities of
interest.  For example, our second definition of the causal effect is
the mean causal effect, which is the average over repeated
hypothetical draws of the the random causal effect:
\begin{align}
  \label{meance}
  \text{(Mean Causal Effect for unit $i$)}
  &= E(\text{Random Causal Effect for unit $i$})\\
  &= E[Y_i(1) - Y_i(0)]\\ \notag &= \mu_i(1) - \mu_i(0),
\end{align}
where $E[Y_i(1)]\equiv\mu_i(1)$ and $E[Y_i(0)]\equiv\mu_i(0)$.

\paragraph{Average Quantities of Interest}
In most applications, we do not attempt to estimate the treatment
effect for each observation, and instead estimate the average over all
observations or some subset of observations.  This leads to two
choices for quantities of interest, for either realized and random
causal effects (we show them here for the latter).  First is the
population average treatment effect or ATE:
\begin{align}
  \label{pate}
  \text{ATE} = E[Y_i(1) - Y_i(0)] \\
  = \frac{1}{n}\sum_{i=1}^n[\mu_i(1) - \mu_i(0)],
\end{align}
which is the mean causal effect for unit $i$ averaged over all units
(so that the expected value operator in the first line averages over
the random potential outcomes for each unit as well as over units).

The alternative choice of a target quantity of interest is the average
treatment effect on the treated, or ATT:
\begin{align}
  \label{att}
  \text{ATT} = E[Y_i(1) - Y_i(0)|t_i=1] \\
  = \frac{1}{\sum t_i}\sum_{i:t_i=1}[\mu_i(1) - \mu_i(0)],
\end{align}
In our running example, this is the average causal effect in districts
in which the Democratic Party nominated the incumbent member of the
House.  From one perspective, we might want to know this treatment
effect on the treated (the ATT) since obviously this is the group of
districts where the treatment was applied.  In other words, the ATT is
the effect of the treatment actually applied.  Medical studies
typically use the ATT as the designated quantity of interest because
they often only care about the causal effect of drugs for patients
that receive or would receive the drugs.  For another example, in job
training programs, we are not normally interested in assinging
employed people to have this training.  In the social sciences, the
ATT is also a reasonable choice, but so is the average treatment
effect (the ATE).  In our running example, it is quite likely that if
an incumbent were nominated in districts in which he or she did not
actually receive the nomination that an incumbency advantage would
still exist, and whatever this effect is, it might be of substantive
interest.  In this paper, we usually focus on ATT as the quantity of
interest when it is conceptually or algebraically simpler, but we also
show how to compute the ATE.

\section{Data Collection Mechanisms}

We now describe the assumptions necessary for making causal inferences
in experimental and observational research.  Some version of these
assumptions, or some way to deal with the information in them, are
necessary no matter what statistical methods are used for estimation.
Any specific statistical method chosen will make additional
assumptions, but the ones discussed here affect essentially all
methods.

\subsection{Experimental}

Although true experiments are only very rarely conducted in political
science, they remain a useful ideal type for understanding other types
of research.  In addition, some of the assumptions necessary for valid
inference in experiments will be approximated by the preprocessing
procedures we suggest below.

Valid and relatively automatic causal inferences can be achieved via
classical randomized experiments (where by ``classical'' we refer to
the situation where randomness is complete and not merely applied
within categories of other variables).  Such experiments have three
critical features: (1) \emph{random selection} of units from a given
population to be observed, (2) \emph{random assignment} of values of
the treatment ($t_i$) to each observed unit, and (3) a \emph{large
  $n$}.

The first feature avoids selection bias by identifying a given
population and guaranteeing that the probability of selection from
this population is related to the dependent variable only by random
chance.  Combining this with the large $n$ from the third feature
guarantees that the chance that something will go wrong is vanishingly
small.

Random assignment in feature (2) guarantees the absence of omitted
variable bias even without any control variables included.  To see
this, recall that, under the usual econometric rules for omitted
variable bias, a variable $X_i$ must be controlled for if it is
causally prior to $t_i$, empirically related to $t_i$, and affects
$y_i$ conditional on $t_i$.  If, instead, one or more of the three
conditions do not hold, then $X_i$ may be omitted without any bias
resulting.  Random assignment guarantees that $t_i$ is unrelated to
\emph{any} $X_i$, whether measured or not, except by random chance.
Moreover, the large $n$ in condition (3) guarantees that this chance
is vanishingly small.

Experiments are a true ideal type, particularly in relation to most
social science research where almost all research fails to meet at
least one of the three features.  Even most social science lab
experiments have random assignment but no random selection and often a
small $n$.  Traditional survey research has what is intended to be
random selection (although with dramatically increasing nonresponse
rates and cell phone usage, this is a less plausible claim) and
certainly has a large $n$, but random assignment, except when the
treatment is the wording of survey questions, is usually impossible.

\subsection{Observational}

Observational data was generated in a way that does not meet all three
of the features of a classical randomized experiment.  Researchers
trying to use the experimental paradigm try to design research to meet
all three features discussed in the previous section.  Researchers
analyzing observational data are instead forced to make assumptions
that, if correct, help them avoid various threats to the validity of
their causal inferences.

In this paper, we shall assume data are selected in a manner that does
not generate selection bias.  Observations need not be selected at
random, as in an experiment, but the probability of selection must not
be correlated with the dependent variable $y$, after taking into
account the treatment $t$ and pre-treatment covariates, $X$.  This is
not a trivial matter, and it is the subject of a great deal of concern
and study in a large variety of methodological and substantive
literatures.  We mention it here to emphasize that all the well-known
concerns in the literature about selecting on the dependent variable
should remain a concern to researchers even when adopting our approach
to preprocessing data via nonparametric matching procedures.

We also assume that a researcher analyzing observational data has
sufficient information in their measured pre-treatment control
variables $X$ so that it is possible via \emph{some} method to make
valid causal inferences.  This is known in some fields as the absence
of omitted variable bias, so that $X_i$ must include all variables
that are causally prior to $t_i$, associated with $t_i$, and affect
$y_i$ conditional on $t_i$.  In other fields, this same condition is
known as ignorability, which means that $t_i$ and the potential
outcomes are independent after conditioning on $X_i$, and so we can
literally ignore all unobserved variables.  Ignorability is a strong
condition, but it is one about which social scientists are deeply
knowledgeable and which is the central methodological concern of many
substantive scholarly articles.  We emphasize this assumption to make
clear that our procedures contain no magic: They are unable to control
for variables that are not measured.

Assuming ignorability and the absence of selection bias still leaves
many assumptions to be made when choosing a specific statistical
inference method.  Results from these methods can still be biased,
unbiased, efficient, inefficient, and almost anything else, depending
on the analyst's choices.  We now focus on this point in the context
of commonly used parametric methods.

\section{Parametric Analysis Methods}

Researchers in the situation of knowing, up to but not including some
unknown parameters, the parametric model that generated their data
should specific and directly estimate this parametric model.
Preprocessing with matching procedures is suboptimal in this
situation.  Of course, few researchers with real observational data
sets in the social sciences have this kind of knowledge and as a
result some choices need to be made among the range of possible
parametric models.

We thus begin by specifying a single but general parametric model that
characterizes the range of models that researchers might choose from.
Indeed, the special cases of thsi model include almost all parametric
models that have been used in the social sciences.  First define the
stochastic component as $Y_i \sim p(\mu_i,\theta)$, for a specified
probability density $p(\cdot)$, mean $\mu_i$, and vector of ancillary
parameters $\theta$.  Then denote the systematic component as
$E(Y_i|t_i,X_i)\equiv\mu_i=g(\alpha + t_i\beta + X_i\gamma)$, for some
specified functional form $g(\cdot)$ and with intercept $\alpha$ and
coefficients $\beta$ and $\gamma$.  The ancillary parameters may also
be specified to vary over observations as a function of $X_i$ or other
covariates.  This framework includes all ``generalized linear models''
as well as many others.  For example, if $p(\cdot)$ is normal and
$g(c)=c$, we have linear regression; if $p(\cdot)$ is Bernoulli and
$g(c)=1/(1+e^{-c})$, the model reduces to a logistic regression.

We define the ATT in (\ref{att}) under this model by plugging in the
definitions of the potential outcomes from the systematic component,
with $t_i$ taking on values 1 and 0 respectively:
\begin{align}
  \label{matt}
  E(Y_i(1)|t_i=1,X) &\equiv \mu_i(1) = g(\alpha + \beta + X_i\gamma)\notag \\
  E(Y_i(0)|t_i=0,X) &\equiv \mu_i(0) = g(\alpha + X_i\gamma)
\end{align}
We can produce estimates of these quantities by assuming independence
over observations, and forming a likelihood function
\begin{equation}
  \label{lik}
  L(\alpha,\beta,\gamma,\theta|y) = \prod_{i=1}^n 
  p\left(y_i \mid g(\alpha + t_i\beta + X_i\gamma), \theta\right)
\end{equation}
the maximum of which gives parameter estimates.

We now turn to the difficulties in making causal inferences from
experimental vs.\ observational data under this general model.

\subsection{Experimental Data}\label{s:paraexp}

In experimental data, classical random assignment guarantees that
$T_i$ and (any observed or unobserved) $X_i$ are independent.  Since
stochastic independence implies mean independence, we can drop $X$ and
write $E(Y_i(1)|t_i=1,X)=E(Y_i(1)|t_i=1)$ and
$E(Y_i(0)|t_i=1,X)=E(Y_i(0)|t_i=1)$.  As such, we can simplify
(\ref{matt}) as
\begin{align}
  \label{smatt}
  E(Y_i(1)|t_i=1) &\equiv \mu_i(1) = g(\alpha + \beta)\notag \\
  E(Y_i(0)|t_i=0) &\equiv \mu_i(0) = g(\alpha)
\end{align}
and so the average treatment effect on the treated from (\ref{att})
becomes simply
\begin{equation}
  \label{satt}
  \text{ATT} = g(\alpha+\beta) - g(\alpha),
\end{equation}
which most importantly no longer has a summation sign over $i$.

The systematic components in Equations \ref{smatt} are now scalar
constants for all $i$.  This is a key result, since it means that
changing the definition of the functional form $g(\cdot)$ now amounts
to nothing more than a simple reparameterization.  Fortunately,
maximum likelihood is invariant to reparameterization --- meaning, for
example, that the MLE of $\alpha$ is the same as the square root of
the MLE of $\alpha^2$ \citep[][p.75-6]{King89} --- which implies that
no matter how $g(\cdot)$ is defined we get the same estimate of the
expected potential outcomes.\footnote{To rule out degenerate cases
  such as $g(a)=8$, we require that the image of $g(\cdot)$ and the
  potential outcomes be the same.} Moreover, given any chosen
stochastic component, this result holds for virtually any parametric
model, including all the special cases of the general model given
above.

Since the specific maximum likelihood estimator of the population mean
for most common probability densities is merely the sample mean, the
analysis of classical randomized experiments often comes down to
taking the difference in the sample means of $y$ for the treatment and
control groups.  But even if one chooses to run a parametric model,
the absence of model dependence means that it will not matter: The
results will be the same no matter what the choice for $g()$ is.

\subsection{Observational Data} \label{s:paraobs}

In experiments, then, random assignment breaks the link between $T$
and $X$ and, by so doing, eliminates the problem of model dependence.
When analyzing observational data with parametric methods we are not
so fortunate.  We cannot reduce Equations \ref{att} to Equations
\ref{matt} and so are left having to model the full functional
relationship that connects the mean as it varies as a function of $t$
and $X$ over observations.  Since $X$ is typically multidimensional,
this is a task far more difficult and highly dependent on often
unstated assumptions than many realize.

To see this point, which is known as the curse of dimensionality,
suppose for simplicity that we have a dependent variable and one
ten-category explanatory variable, and our goal is use linear
regression technology to estimate the functional relationship without
functional form assumptions.  To do this, we would represent the ten
categories with ten parameters (a constant and nine dummy variables or
10 mean indicator variables).  In contrast, the usual approach to
estimation is to assume linearity.  This enables us to enter not 10
indicator variables, but rather only a constant term and one slope
coefficient.  How do we get from ten parameters to only two?  Pure
assumption about the remaining eight.  If we have some sense that the
relationship is indeed linear or close to linear, this is a good use
of external information to reduce the number of parameters that must
be estimated.  If not, then we still have the best linear
approximation to the conditional expectation function, but the
relationship we estimate can be far off.  If we are running this
regression for the purpose of estimating a causal effect, then the
treatment variable is also in the regression, and its coefficient can
be biased to any degree if the functional relationship with the
control variables is misspecified.

This problem quickly becomes more serious as the number of explanatory
variables increase.  For example, estimation without functional form
assumptions with two ten-category explanatory variables would require
not 20 parameters but 100.  In this case, the usual approach would
include a constant term and two slope coefficients, reducing 100
parameters to three by pure assumption.  And with multiple explanatory
variables, claims about external knowledge constraining the functional
form much become dubious.  In this example, by what theory would we
know that 97 parameters, representing every form of nonlinearity and
interaction, should be set to exactly zero?  Including a linear
interaction would not help much since it would merely add one more
parameter to estimate, and so we would still need to make assumptions
about the remaining 96 parameters.

The problem escalates very fast as the number of explanatory variables
increase.  With $v$ 10-category variables, we need $10^v$ parameters
for estimation in a regression framework without functional form
assumptions.  So five explanatory variables leads to a regression
model with 100,000 parameters, and the usual approach works by
estimating only a constant term and five slope coefficients and
restricting by assumption the remaining 99,994 parameters.  Nine such
explanatory variables leaves us with a model with one \emph{billion}
parameters, to be approximated by only the ten that we would estimate
under the traditional approach.  Eighty ten-category explanatory
variables would require a regression model with more parameters than
current estimates of the number of elementary particles in the
universe.

Estimating rather than making assumptions about all these extra
parameters is obviously not possible under the standard regression
approach, since social data sets do not come with anywhere near enough
observations.  We cannot avoid the problem with nonlinear or
non-normal statistical models, since these pose the same curse of
dimensionality as linear regression.  The assumption of ignorability,
which enables us to make the positivist assumption that we have have
measured and observe all necessary variables, is insufficient.

Instead, we are led to the inescapable conclusion that, in parametric
causal inference of observational data, many assumptions about many
parameters are frequently necessary, and only rarely do we have
sufficient external information to make these assumptions based on
genuine knowledge.  The frequent unavoidable consequence is high
levels of model dependence, but no good reason to choose one set of
assumptions over another.

Some researchers surely respond to this diversity of possible models
by inadvertently choosing specifications that support their favored
hypotheses.  The best scholars forthrightly portray at least some
aspects of specification uncertainty in their published work by
displaying the results from multiple specifications and evaluating and
emphasizing how model dependent their substantive results are.  But
researchers rarely portray the full sensitivity of their causal
inferences to model specification, and attempts to do so in many
situations lead to nihilistic conclusions.\footnote{Two approaches to
  this problem include extreme bounds analysis \citep{Leamer78}, which
  is a relatively standardized way to portray model dependence, and
  Bayesian model averaging \citep{HoeMadRaf99,ImaKin04}, which is
  intended to draw inferences by appropriately combining inferences
  from models with different specifications.}

\section{Nonparametric Preprocessing} \label{s:nonparpreproc}

The goal of matching in general and our specific nonparametric
preprocessing approach is to adjust the data prior to the parametric
analysis so that (1) the relationship between $t_i$ and $X_i$ is
eliminated or reduced and (2) no bias is induced.  If we are able to
adjust the data so that $t_i$ and $X_i$ are completely unrelated, so
that the control and treatment groups are identical with respect to
$X$ before the treatment assignment, we will have moved a good deal of
the way from Section \ref{s:paraobs} to Section \ref{s:paraexp}.  An
assumption of ignorability will still be necessary, but we will no
longer need to model the full parametric relationship between the
dependent variable and the multidimensional $X_i$.  We will also have
eliminated an important source of model dependence in the resulting
parametric analysis stemming from the functional form specification
and the curse of dimensionality.  For data sets where preprocessing
reduces the extent of the relationship between $t_i$ and $X_i$, but is
unable to make them completely independent, model dependence is not
eliminated but will normally be greatly reduced.  Indeed, if
nonparametric preprocessing results in no reduction of model
dependence, then it is likely that the data have little information to
support causal inferences by any method, which of course would also be
useful information.

But how can we adjust the data without inducing bias in our causal
estimates?  The key to this problem is that the fundamental rule for
avoiding selection bias --- do not select on the dependent variable
--- does not prevent us from selecting observations on the explanatory
variables ($t_i$ or $X_i$).  Random or other physical assignments in
experiments and stratified sampling in surveys are examples of data
collection mechanisms that select observations given chosen values of
the explanatory variables.  But we can also select observations, or
selectively drop observations from an existing sample without bias, as
long as we do so using a rule that is a function only of $t_i$ and
$X_i$.  Our preprocessed dataset will therefore include a selected
subset of the observed sample for which $t_i$ and $X_i$ are unrelated,
or in other words for which this relationship holds:
\begin{equation}
  \label{balance}
  p(X|t=1) = p(X|t=0).
\end{equation}

The simplest way to satisfy (\ref{balance}) is to use \emph{one-to-one
  exact matching}.  The idea is to match each treated unit with one
control unit for which the values of $X_i$ are identical.  Our
preprocessed dataset thus is the same as the original dataset with any
unmatched units discarded, and $t_i$ and $X_i$ are now independent.
If enough matches are available, this procedure eliminates all
dependence in the parametric analysis on the functional form.  It is
also highly intuitive, since it directly parallels an experiment where
we find pairs of units that are identical in all observable ways and
assign one from each pair to be treated and the other to be a control.
Then no matter what effect $X_i$ has on $y$, we can ignore it entirely
since $X_i$ is literally held constant within each pair of units.

Although one-to-one exact matching can eliminate model dependence and
any bias from incorrect assumptions made during the parametric stage
of analysis, it has the disadvantage in many applications of not
generating many matches.  The problem is of course most severe if
$X_i$ is high dimensional or contains continuous variables.  The
result may then be a preprocessed data set with very few observations
that results in a parametric analysis with large standard errors.  In
this situation, we may have reduced model dependence and the potential
for bias but decreased efficiency and as a result increased mean
square error.  If this occurs in practice, those in the matching
literature tend to sacrifice some bias reduction for the increased
efficiency that comes from having more observations in the
preprocessed dataset.  In our approach, sacrificing bias reduction
only affects the preprocessing stage, and our second stage parametric
analysis has a chance to eliminate the remaining bias.  We therefore
now turn to a menu of matching procedures that enable researchers to
satisfy (\ref{balance}) as closely as possible while still generating
a preprocessed dataset with enough observations.

\section{Choosing a Matching Procedure}

\section{Computing Uncertainty Estimates}

No concensus exists in the literature on methods to use to compute
uncertainty estimates, such as standard errors or confidence
intervals, from matching procedures.  The problem is not some
disagreement over technical statistical techniques.  Rather, the issue
revolves around normative criteria such as what researchers should
condition on and what they should attribute to additional uncertainty.
Since scholars are no more likely to reach consensus via debate on
normative statistical issues than on normative political issues, we
believe the best way forward is to choose a reasonable definition and
show how to compute uncertainty estimates for it, and let others pick
different definitions if they prefer.  Our choice, which we now
explicate, appears substantively reasonable and has the advantage of
being easy to implement.

Parametric methods applied to raw data without preprocessing come with
a list of relatively standard ways of computing uncertainty estimates.
These include methods based on using the asymptotic normal
approximation to the likelihood function, direct simulation from the
finite sampling distribution or posterior density, various frequentist
bias corrections, robust Bayesian analysis involving classes of
posteriors, and even nonparametric bootstrapping, among others.  Any
of these are easy to implement by drawing and summarizing simulations
of the chosen quantity of interest, given a parametric model and
accompanying theory of inference.

Our idea is to take advantage of a common feature of the uncertainty
estimates associated with parametric methods: They are all conditional
on the pretreatment variables $X$ (and $t$), which are therefore
treated as fixed and exogenous.  Since our preprocessing procedures
modify the raw data only in ways that are solely a function of $X$, a
reasonable method of defining uncertainty for the purpose of computing
uncertainty estimates is to continue to treat $X$, and thus our entire
preprocessing procedures, as fixed.  The advantage of this definition
is that we can easily compute standard errors and confidence intervals
using the same methods researchers have been using with their
parametric methods all along, but applied to the preprocessed instead
of raw data.  (The one exception to our procedure occurs when matching
with replacement, where we would then run the parametric procedure
weighted by the number of treated units matched to each control unit.)

Thus, when estimating the ATT or ATE, we compute estimates of
$\mu_i(1)$ and $\mu_i(0)$ and their uncertainty as usual from the
parametric model applied to the preprocessed data.  If computing the
realized causal effect (either on average over all observations or
just the average for the treated units), we compute parametric
estimates conditional on the observed $y_i$ for each unit.  In this
situation, if $t_i=1$, we set $\mu_i(1)=y_i$ and use the parametric
model to estimate $\mu_i(0)$ and its uncertainty, whereas if $t_i=0$,
we set $\mu_i(0)=y_i$ and use the parametric model to estimate
$\mu_i(1)$ and its uncertainty estimate.  In Bayesian models,
estimating the realized causal effect requires the equivalent of
conditioning on $y_i$ after the likelihood estimation \citep[as
in][]{King97}.

\section{Empirical Illustrations}

In this section, we provide empirical illustrations of how conceiving
of matching as preprocessing can reduce the sensitivity of inferences.
Social scientists are often faced with a perplexing array of
specification choices as discussed in Section~\ref{s:paraobs}.  Our
goal here is to demonstrate that matching as preprocessing has the
potential to reduce model sensitivity to such specification choices.
The result is that researchers may be able to estimate effects with
unjustified modeling assumptions playing a decreased role.  We define
sensitivity as the variability in point estimates across potential
specifications of pre-treatment covariates.  In these illustrations we
concern ourselves with bias, not efficiency, although the latter has
been the focus of much attention in the economic literature (cites).

\subsection{Assessing the Causal Effect of Visibility on Candidate
  Evaluations}

In our first illustration we study citizen evaluations of ideological
position for candidates for the House of Representatives in the 1990s.
This data was first studied by \citet{Koch02} and we were able to
replicate substantially all of the results therein.\footnote{In our
  version of the dataset, political awareness is scaled slightly
  differently, a matter of no substantive significance that changes
  none of the results.  We thank Jeff Koch for providing the data and
  for clarifying this nominal difference.}  The treatment of interest
is the effect of being a highly visible female candidate on citizen
voter evaluations of the candidate's ideology (scored as a seven point
ordinal point scale,where high scores indicate greater
conservatism).\footnote{Visibility is measured originally by whether a
  candidate had campaign expenditures exceeding more than \$750,000.
  For simplicity and comparison to \citet{Koch02}, we do not concern
  ourselves here with the correctness of these measurements, nor do we
  seek to improve the methodology of studying an ordinal dependent
  variable.}  We confine ourselves to studying this effect on
Republican female candidates, compared to the control of invisible
male and female candidates.\footnote{This corresponds to the fourth
  column of Table 2 in \citet[p.  459]{Koch02}.}  Understanding the
micro-mechanisms of how citizens form impressions of candidates is of
great import to the literature on voting and information \citep[see,
e.g.,]{Bartels96,Popkin94}, yet field experiments that randomly assign
visibility to candidates are difficult to conduct for obvious reasons.

As a result, \citet{Koch02} collects observational data, including
potential confounding covariates of candidate ideology, voter
perception of party ideology, respondent ideology, feeling
thermometer, and political awareness.  Using ordinary least squares to
control for these covariates, the idea is that we can hold constant
all of these explanatory variables, varying only the treatment.  Yet,
we can quickly see that even with such a small number of covariates,
the curse of dimensionality looms large.  Given unique values of these
covariates, nonparametric estimation would require estimation of $4.4
\times 10^7$ parameters, yet the full dataset contains only 494 units.
So instead, \citet{Koch02} estimates the treatment effect by
assumption, employing ordinary least squares.

Yet as the first three columns in Table~\ref{tb:kochmtest} show,
visible and invisible candidates differ drastically in explanatory
covariates.  Most strikingly, visible female candidates tend to be
substantially more liberal than invisible candidates: visible
candidates are rated 0.44 on a scale from 0 to 1, which is almost one
standard deviation higher than the 0.2 average rating of invisible
candidates (t-statistic=12.18).  In addition, visible candidates
appear to be slightly more politically aware (t-statistic=2.42).  As a
result of this covariate imbalance, we may not be comfortable in
reducing $4.4 \times 10^7$ parameters to 6 merely by assumption.
Extrapolation is likely to be significant.  Indeed, model sensitivity
may lead some scholars to inadvertently report regressions that
confirm some hypotheses when other specifications would caution
against such a conclusion.  Merely six explanatory variables, for
example, may enter the right-hand side of a linear regression in 63
different ways (=$\sum_{i=1}^6 {6 \choose i}$), but scholars rarely
report results across all such possible specifications.

\begin{table}[t]
  \begin{center}
    \begin{tabular}{lrrrrrr}
      \hline
      & \MC{3}{c}{\bf Raw Data} & \MC{3}{c}{\bf Pre-processed Data
      (subclass 4) }\\
      & Mean for  & Mean for  &    & Mean for  & Mean for \\
      & Treated & Control  & t-stat &    Treated & Control  & t-stat \\
      \hline
      Propensity Score & 0.48 & 0.10 & 10.71 & 0.61 & 0.54 & 2.32 \\
      Candidate Ideology & 0.44 & 0.20 & 12.18 & 0.50 & 0.48 & 1.15 \\
      Perception of Party Ideology & 0.72 & 0.73 & -0.42 & 0.69 & 0.67 & 0.24 \\
      Respondent Ideology & 0.49 & 0.53 & -0.97 & 0.45 & 0.50 & -0.38 \\
      \hspace{0.1in} Respondent Ideology $\times$ & 0.31 & 0.32 & -0.44 & 0.27 &
      0.25 & 0.18 \\
      Feeling Thermometer\\
      Feeling Thermometer & 0.60 & 0.55 & 1.70 & 0.54 & 0.53 & 0.03 \\
      Political Awareness & 0.76 & 0.70 & 2.42 & 0.76 & 0.74 & 0.28 \\
      \hline
    \end{tabular}
    \caption{Imbalance of pre-treatment covariates in the Koch data.
      The full sample size was 494, and the matched dataset with
      subclassification was 463.  The first three columns present
      means for visible female (``treated'') and invisible
      (``control'') Republican candidates and t-statistics for the
      full dataset.  The last three columns present means and
      t-statistics for one representative subclass.  This table shows
      that candidate ideology is highly imbalanced in the full Koch
      data, and that balance can be achieved by subclassification.
      Visible females, for example, were much more likely to be
      perceived as liberal than invisible candidates overall, but
      within subclass 4 this difference diminishes.}
    \label{tb:kochmtest}
  \end{center}
\end{table}

Preprocessing by matching reduces the imbalance of covariates so that
parametric assumptions play a smaller role in estimation.  We hence
match by subclassification on the propensity score.  The procedure is
quite simple, and easily implemented and further demonstrated in our
accompanying software and documentation.  (In addition, we investigate
other matching techniques, such as one-to-one nearest neighbor
matching and caliper matching, which yield the same substantive
results.)  We estimate the propensity score with a logistic
regression, using all six pre-treatment covariates as
predictors.\footnote{Throughout this application, we treat respondent
  ideology $\times$ feeling thermometer as one covariate as in
  \citet{Koch02}.}  We then discard treatment units whose propensity
score is greater than the maximum of control units and control units
whose propensity score is lower than the minimum of the treated units.
This results in 31 discarded units, and ensures that outliers do not
contaminate our data.  Next, we create 6 subclasses by quantiles of
the propensity score of the treated units.  As is desired, within each
subclass, covariate balance is substantially better than in the full
sample.  For example, the last three columns of
Table~\ref{tb:kochmtest} presents means and t-statistics for each
covariate in one representative subclass showing that balance
uniformly increases in each of these covariates.  Candidate ideology,
which was highly imbalanced in the raw data, is now 0.5 for visible
candidates within subclass 4, which differs only slightly from the
0.48 for invisible candidates in that subclass (t-statistic=1.15).

\begin{table}[t]
  \begin{center}
    \begin{tabular}{lrrrrrr}
      \hline
      & \MC{6}{c}{\bf Subclass} \\
      Covariate &  1 &  2 &  3 &  4 &  5 &  6 \\
      \hline
      Candidate Ideology & 2.16 & -0.85 & 0.33 & 1.15 & 1.51 & -0.45 \\
      Perception of Party Ideology & -0.65 & -1.05 & 0.59 & 0.24 & 0.06 & 1.53 \\
      Respondent Ideology & 1.45 & -0.73 & 1.08 & -0.38 & -1.26 & -1.00 \\
      Respondent Ideology $\times$ & 0.50 & -0.81 & 1.64 & 0.18 &
      -1.44 & -0.42 \\
      \hspace{0.1in} Feeling Thermometer \\
      Feeling Thermometer & -0.03 & 0.33 & 1.61 & 0.03 & -1.34 & 1.08 \\
      Political Awareness & 0.53 & 0.14 & -1.50 & 0.28 & -1.08 & 0.66 \\
      No. treated & 14.00 & 13.00 & 13.00 & 14.00 & 13.00 & 11.00 \\
      No. control & 299.00 & 30.00 & 39.00 & 12.00 & 3.00 & 2.00 \\
      $N$ & 313.00 & 43.00 & 52.00 & 26.00 & 16.00 & 13.00 \\
      \hline
    \end{tabular}
    \caption{Means test statistics for six pre-treatment covariates.
      Each cell presents t-statistic for the covariate within each
      subclass, and No. treated / control indicates the number of
      treated / control units in each subclass and $N$ indicates the
      total number of units in each subclass.  This table shows that
      relative balance can be achieved within each subclass with only
      six subclasses.}
    \label{tb:kochxsub}
  \end{center}
\end{table}

To check balance for all covariates in all of the subclasses,
Table~\ref{tb:kochxsub} presents simple means test statistics for each
of the main covariates in each of the subclasses.  The last three rows
present the number of unit in each subclass.  Note that there are few
visible candidates matched with many more invisible candidates in the
lower subclasses and the reverse in the higher subclasses.  This
indicates that in the raw data, covariates are quite unbalanced.
Through this simple technique we are able to compare relatively
similar candidates within each subclass.  Indeed across
subclass-covariates the t-statistic exceeds 2 only once, which is
roughly the same as we would expect if the data were generated
randomly.

\begin{figure}[t] 
 \begin{center}
    \includegraphics[height=3.5in,angle=0]{figs/kochdens.pdf}
  \end{center}
  \vspace{-0.275in}
  \caption{Density estimates of estimated effects of being
    a highly visible female Republican candidate across 63 possible
    specifications with the Koch data.  The solid line presents
    estimates for the full dataset and the dashed line presents
    estimates for the matched dataset (via subclassification).  The
    vertical arrow presents the point estimate of the regression
    presented in the original paper.  This figure shows that treatment
    effect estimates are much more sensitive to model specification
    for the full dataset compared to the matched dataset.}
  \label{fg:kochdens}
\end{figure}

We now assess the claim of model sensitivity by reporting treatment
effect estimates of parametric models with the full dataset and our
matched (subclassified) dataset.  Specifically, we consider all 63
possible combinations in which the 6 covariates may enter the
regression with the treatment indicator.  For each specification, we
record the point estimate of the treatment coefficient representing
the average effect of being a visible female (Republican) candidate on
the voter's ideological assessment.  For the matched dataset, we run
the exact same specification locally within each subclass, aggregating
the point estimates weighed by the number of treated units in each
subclass.  The result is a distribution of treatment effect estimates
across possible specifications for raw and pre-processed data.

Figure~\ref{fg:kochdens} presents the results, plotting densities of
estimated treatment effects for the raw and pre-processed data.  As
hypothesized, the range of treatment effect estimates is substantially
more variable for the raw data.  Effect estimates, plotted by the
solid line, range from $-0.64$ to $-0.2$, signifying female visibility
causes voters to perceive candidates as more liberal.  The point
estimate from the full data that controls for all six covariates,
signified by the vertical arrow, appears conservative compared to this
range.  Yet estimates from the pre-processed data stand in stark
contrast the to estimates from the raw data.  The range of coefficient
estimates is substantially smaller, ranging from $-0.31$ to $-0.07$,
and the variance across specifications is more than six times smaller
than for the raw data, as shown by the highly peaked density.  Rather
than erring on the conservative side, the reported point estimate now
appears to be substantially larger in substantive effect than we might
expect from the pre-processed data.

In short, not only can pre-processing reduce model dependence,
decreasing the probability that researchers may inadvertently present
unrepresentative estimates, but pre-processing also may change
substantive conclusions of the research.  The fact that treatment
effects may be substantially smaller may even further bolster the
claim that ``[t]he contradictory nature of . . . [the] cues received
from Republican female candidates presents citizens with a complicated
information-processing task'' \citep[p. 460]{Koch02}.

\section{What Can Go Wrong}

\section{Concluding Remarks}

\appendix
\section{Matching Software}

We have created software called MatchIt for the free and open source
package R (see http://www.r-project.org).  MatchIt, which is available
at http://gking.harvard.edu/matchit/, implements a large fraction of
the matching procedures suggested in the diverse scholarly literatures
on this subject.  Most of our software operates with a single command
that takes an existing dataset and produces as output a single
preprocessed matched dataset.  The preprocessed matched dataset can
then be used by other software just as you would have used the
original dataset.

For example, to use propensity score matching of a treatment
indicator, \texttt{treat}, and three pretreatment covariates, we could
use this command:
\begin{verbatim}
match.out <- matchit(treat ~ age + educ + black, data = lalonde)
\end{verbatim}
and to exact match use:
\begin{verbatim}
match.out <- matchit(treat ~ age + educ + black, exact=TRUE, data = lalonde)
\end{verbatim}
Numerous other options are also available.

MatchIt also works well with the general purpose statistics package
Zelig (see http://gking.harvard.edu/zelig).  For example, suppose we
first used MatchIt to subclassify into 5 classes by the propensity
score, use
\begin{verbatim}
match.out <- matchit(treat ~ age + educ + black, subclass=5, data = lalonde)
\end{verbatim}
Then we could use Zelig to estimate a parametric model (in this case
least squares regression) within each of the five subclasses defined
by MatchIt, using data from the control group:
\begin{verbatim}
z.out <- zelig(re78 ~ pscore, model = "ls", by = "psclass", 
                data = match.data(match.out, "control"))
\end{verbatim}
and then set the explanatory variables to predict the potential
outcomes for the treatment group and simulate quantities of interst:
\begin{verbatim}
x.out <- setx(z.out, data = match.data(match.out2, "treat"), cond = TRUE)
s.out <- sim(z.out, x = x.out)
\end{verbatim}


\baselineskip=0.637\baselineskip \bibliographystyle{apsr}
\bibliography{gk,gkpubs}

\end{document}
